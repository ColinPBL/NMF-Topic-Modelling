{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIUFxsn7IVrL"
   },
   "outputs": [],
   "source": [
    "def calculate_date_interval(df, figsize):\n",
    "    \"\"\"\n",
    "    Calcule l'intervalle dynamique pour les dates sur l'axe des abscisses.\n",
    "    \"\"\"\n",
    "    # Calcul de la largeur totale en pixels\n",
    "    fig_width, fig_height = figsize\n",
    "    dpi = plt.gcf().dpi\n",
    "    total_width_px = fig_width * dpi\n",
    "\n",
    "    # Estimation de la largeur d'une date en pixels\n",
    "    sample_date = df.index[0].strftime('%d-%m-%Y')\n",
    "    plt.figure()\n",
    "    temp_text = plt.text(0.5, 0.5, sample_date)\n",
    "    plt.draw()\n",
    "    bbox = temp_text.get_window_extent()\n",
    "    date_width_px = bbox.width\n",
    "    plt.close()\n",
    "\n",
    "    date_width_px = date_width_px / 4\n",
    "\n",
    "    # Calcul de l'intervalle\n",
    "    max_dates_without_overlap = total_width_px / date_width_px\n",
    "    interval = max(1, int(np.floor(len(df.index) / max_dates_without_overlap)))\n",
    "\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwuBl4KqyL6n"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies a transformer-based lemmatisation to each document using several processes (one for each cpu core)\n",
    "\"\"\"\n",
    "def treat_documents():\n",
    "    global documents_lemmatized, all_tab_pos, sentences_norms, sentences_lemmas, language\n",
    "\n",
    "    # Use synchronized queues to safely share data between processes\n",
    "    input_queue = Queue()\n",
    "    output_queue = Queue()\n",
    "\n",
    "    # Démarrage des processus de travail\n",
    "    workers = []\n",
    "    for _ in range(cpu_count()):\n",
    "        p = Process(target=worker, args=(input_queue, output_queue, language, {\n",
    "            \"total_docs\": len(documents)\n",
    "        }))\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "\n",
    "    # Ajouter les documents à la input_queue avec leur index\n",
    "    for idx, doc in enumerate(documents):\n",
    "        input_queue.put((idx, doc))\n",
    "\n",
    "    results = []\n",
    "    # Récupération des résultats avec barre de progression\n",
    "    for _ in tqdm(range(len(documents)), desc=\"DOCUMENTS PROCESSED\", maxinterval=1000, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        results.append(output_queue.get())\n",
    "\n",
    "    # Envoyer le signal d'arrêt aux processus de travail\n",
    "    for _ in workers:\n",
    "        input_queue.put(\"STOP\")\n",
    "\n",
    "    # Attendre la fin de tous les processus\n",
    "    for p in workers:\n",
    "        p.join()\n",
    "\n",
    "    # Trier les résultats en fonction de leur index et agréger les données\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    for result in results:\n",
    "        idx, document_lemmatized, tab_pos, norms, lemmas = result\n",
    "        documents_lemmatized.append(document_lemmatized)\n",
    "        all_tab_pos.append(tab_pos)\n",
    "        sentences_norms.append(norms)\n",
    "        sentences_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k17fyCzmzYDL"
   },
   "outputs": [],
   "source": [
    "def extract_terms_with_scores(spacy_object, nb_candidates_to_extract_from_terms_extractions):\n",
    "    def get_top_terms_with_scores(attribute):\n",
    "        # Extraire les termes et leurs scores\n",
    "        sorted_terms = attribute.sort_values(ascending=False)\n",
    "        top_terms = sorted_terms.head(nb_candidates_to_extract_from_terms_extractions)\n",
    "        return list(top_terms.items())\n",
    "\n",
    "    # Obtenir les n-grammes cvalues avec leurs scores\n",
    "    cvalues_terms = get_top_terms_with_scores(spacy_object._.cvalues)\n",
    "\n",
    "    return cvalues_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NdI3EBkzYDM"
   },
   "outputs": [],
   "source": [
    "def extract_terms(spacy_objects, nb_candidates_to_extract_from_terms_extractions):\n",
    "    cvalues_results = []\n",
    "\n",
    "    for spacy_obj in spacy_objects:\n",
    "        cvalues_terms = extract_terms_with_scores(spacy_obj, nb_candidates_to_extract_from_terms_extractions)\n",
    "        cvalues_results.append(cvalues_terms)\n",
    "\n",
    "    # Accumuler les scores pour chaque multigramme unique\n",
    "    score_accumulation = defaultdict(float)\n",
    "\n",
    "    for sublist in cvalues_results:\n",
    "        for term, score in sublist:\n",
    "            score_accumulation[term] += score\n",
    "\n",
    "    # Trier les multigrammes par leur score cumulé\n",
    "    sorted_terms = sorted(score_accumulation.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Sélectionner les nb_candidates_to_extract_from_terms_extractions multigrammes avec les scores les plus élevés\n",
    "    all_added_ngrams = sorted_terms[:nb_candidates_to_extract_from_terms_extractions]\n",
    "\n",
    "    return all_added_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvNMOepHffb9"
   },
   "outputs": [],
   "source": [
    "def go_tfidf_vectorization(unigrams):\n",
    "    args_list = [(atb,\n",
    "                unigrams) for atb in all_tab_pos]\n",
    "\n",
    "    def parallel_tokenize_multiprocessing(args_list):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            return pool.map(tokenize_and_stem, args_list)\n",
    "\n",
    "    tokenized_documents = parallel_tokenize_multiprocessing(args_list)\n",
    "\n",
    "    #tfidf_vectorizer = TfidfVectorizer(tokenizer=None, norm='l2', sublinear_tf=sub_linear_tfidf) # normalisation pour considérer les tailles différentes de documents\n",
    "\n",
    "    def identity_analyzer(tokens):\n",
    "        return tokens\n",
    "\n",
    "    count_vectorizer = CountVectorizer(analyzer=identity_analyzer, lowercase=False)\n",
    "    word_count = count_vectorizer.fit_transform(tokenized_documents)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(norm='l2', sublinear_tf=sub_linear_tfidf)\n",
    "    X = tfidf_transformer.fit_transform(word_count)\n",
    "\n",
    "    tfidf_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    return count_vectorizer, X, tfidf_feature_names, tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUmECXrmzYDM"
   },
   "outputs": [],
   "source": [
    "def write_sentences_results(out_name, final_top_ngrams_per_topic):\n",
    "    with open(out_name, \"w\", encoding='utf-8') as file_object:\n",
    "      writer = csv.writer(file_object)\n",
    "\n",
    "      # Écrire les en-têtes si nécessaire\n",
    "      headers = []\n",
    "      for i in range(len(final_top_ngrams_per_topic)):\n",
    "          headers.extend([f'{i}_sentences', f'{i}_scores'])\n",
    "      writer.writerow(headers)\n",
    "\n",
    "      # Écrire les données\n",
    "      for i in range(10):\n",
    "          row = []\n",
    "          for sub_array in final_top_ngrams_per_topic:\n",
    "              if i < len(sub_array):\n",
    "                  row.extend(sub_array[i])\n",
    "              else:\n",
    "                  row.extend(('', ''))\n",
    "          writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHHdg_Ocffb9"
   },
   "outputs": [],
   "source": [
    "def write_unigrams_results(nb_words, tfidf_feature_names, out_name, nmf_H):\n",
    "\n",
    "    tab_words_nmf = []\n",
    "    for topic_idx, topic in enumerate(nmf_H):\n",
    "      subtab_words_nmf = []\n",
    "      for i in topic.argsort()[:-nb_words - 1:-1]:\n",
    "        subtab_words_nmf.append([(tfidf_feature_names[i]), topic[i]])\n",
    "\n",
    "      tab_words_nmf.append(subtab_words_nmf)\n",
    "\n",
    "\n",
    "    new_tab_words_nmf = []\n",
    "    for t in tab_words_nmf:\n",
    "      sorted_t = sorted(t, key = lambda x: (-x[1]))\n",
    "\n",
    "      new_tab_words_nmf.append(sorted_t)\n",
    "\n",
    "\n",
    "\n",
    "    max_rows_nb = 0\n",
    "    for to in new_tab_words_nmf:\n",
    "      if len(to) > max_rows_nb:\n",
    "        max_rows_nb = len(to)\n",
    "\n",
    "\n",
    "    with open(out_name, \"w\", encoding='utf-8') as file_object:\n",
    "      writer = csv.writer(file_object)\n",
    "\n",
    "      i = 0\n",
    "      while i < max_rows_nb:\n",
    "        new_row = ''\n",
    "        for to in new_tab_words_nmf :\n",
    "          if i < len(to):\n",
    "            if len(new_row) > 0:\n",
    "              new_row = new_row + ',' + (to[i][0]) + ',' + str(to[i][1])\n",
    "            else:\n",
    "              new_row = (to[i][0]) + ',' + str(to[i][1])\n",
    "\n",
    "        file_object.write(new_row)\n",
    "        file_object.write('\\n')\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMaEdi-BzGBT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports all required libraries for the notebook to function. If a library is not present on the system, it is downloaded instead\n",
    "\"\"\"\n",
    "def download_things():\n",
    "    global folder_path\n",
    "    global language\n",
    "\n",
    "    libraries = {\n",
    "        'seaborn': ('import seaborn', 'pip install seaborn'),\n",
    "        'gensim': ('import gensim', 'pip install gensim'),\n",
    "        'cleantext': ('import cleantext', 'pip install cleantext'),\n",
    "        'adjustText': ('import adjustText', 'pip install adjustText'),\n",
    "        'umap': ('import umap', 'pip install umap-learn'),\n",
    "        'nltk': ('import nltk', 'pip install nltk'),\n",
    "        'sklearn': ('import sklearn', 'pip install -U scikit-learn'),\n",
    "        'scipy': ('import scipy', 'pip install -U scipy'),\n",
    "        'matplotlib': ('import matplotlib', 'pip install -U matplotlib'),\n",
    "        'spacy': ('import spacy', 'pip install spacy'),\n",
    "        'spacy-transformers': ('import transformers', 'pip install spacy-transformers'),\n",
    "        'pyate': ('import pyate', 'pip install pyate'),\n",
    "        'multiprocess': ('import multiprocess', 'pip install multiprocess'),\n",
    "        'bs4': ('import bs4', 'pip install beautifulsoup4'),\n",
    "        'unidecode': ('import unidecode', 'pip install unidecode'),\n",
    "        'statmodels': ('import statmodels', 'pip install statmodels')\n",
    "    }\n",
    "\n",
    "    for library, (import_cmd, pip_cmd) in libraries.items():\n",
    "        print('go', library)\n",
    "        try:\n",
    "            if import_cmd:  # If there's an import command provided\n",
    "                exec(import_cmd)\n",
    "            else:\n",
    "                raise ImportError  # This will force the code to the except block\n",
    "        except ImportError:\n",
    "            result = subprocess.run(pip_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Download the spacy model based on the language selected by the user\n",
    "    if language == 'fr':\n",
    "        %pip install -r ./requirements/requirements_fr.txt\n",
    "    elif language == 'en':\n",
    "        !spacy download en_core_web_trf\n",
    "    elif language == 'es':\n",
    "        !spacy download es_dep_news_trf\n",
    "    elif language == 'de':\n",
    "        !spacy download de_dep_news_trf\n",
    "    elif language == 'ca':\n",
    "        !spacy download ca_core_news_trf\n",
    "    elif language == 'zh':\n",
    "        !spacy download zh_core_web_trf\n",
    "    elif language == 'da':\n",
    "        !spacy download da_core_news_trf\n",
    "    elif language == 'ja':\n",
    "        !spacy download ja_core_news_trf\n",
    "    elif language == 'sl':\n",
    "        !spacy download sl_core_news_trf\n",
    "    elif language == 'uk':\n",
    "        !spacy download uk_core_news_trf\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ferTRgwzYDN"
   },
   "outputs": [],
   "source": [
    "def determine_nmf(initial_topic_num=5, terminal_topic_num=20):\n",
    "    global all_nmf_H\n",
    "    global all_nmf_W\n",
    "    global relevant_i_by_topics\n",
    "    global every_topics_and_scores_by_document\n",
    "\n",
    "    for num_topic in tqdm(range(initial_topic_num, terminal_topic_num+5, 5),\n",
    "                          bar_format=\"{l_bar}{bar:10}{r_bar}\",\n",
    "                          desc=\"TOPICS CONFIGURATIONS PROCESSED\"):\n",
    "        relevant_i_by_topics[num_topic] = {}\n",
    "\n",
    "        nmf_model = NMF(n_components=num_topic, random_state=1, max_iter=2000, l1_ratio=0.5, init='nndsvd').fit(tfidf)\n",
    "        nmf_W = nmf_model.transform(tfidf)\n",
    "        all_nmf_W[num_topic] = nmf_W\n",
    "        nmf_H = nmf_model.components_\n",
    "        all_nmf_H[num_topic] = nmf_H\n",
    "\n",
    "        all_topics_and_scores_by_document = {}\n",
    "\n",
    "        for i in range(len(nmf_W)):\n",
    "            topics_and_scores = [(topic_num, nmf_W[i][topic_num]) for topic_num in range(nmf_W[i].shape[0])]\n",
    "            topics_and_scores_sorted = sorted(topics_and_scores, key=lambda x: x[1], reverse=True)\n",
    "            all_topics_and_scores_by_document[i] = topics_and_scores_sorted\n",
    "\n",
    "            if topics_and_scores_sorted[0][0] in relevant_i_by_topics[num_topic]:\n",
    "                relevant_i_by_topics[num_topic][topics_and_scores_sorted[0][0]].append([i, topics_and_scores_sorted[0][1]])\n",
    "            else:\n",
    "                relevant_documents_array = []\n",
    "                relevant_documents_array.append([i, topics_and_scores_sorted[0][1]])\n",
    "                relevant_i_by_topics[num_topic][topics_and_scores_sorted[0][0]] = relevant_documents_array\n",
    "\n",
    "        every_topics_and_scores_by_document[num_topic] = all_topics_and_scores_by_document\n",
    "\n",
    "    for num_topic in relevant_i_by_topics:\n",
    "        for topic in relevant_i_by_topics[num_topic]:\n",
    "            sorted_data = sorted(relevant_i_by_topics[num_topic][topic], key=lambda x: x[1], reverse=True)\n",
    "            top_10 = sorted_data[:10]\n",
    "            indices_top_10 = [item[0] for item in top_10]\n",
    "            relevant_i_by_topics[num_topic][topic] = indices_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiumkOGgzYDN"
   },
   "outputs": [],
   "source": [
    "def terms_and_entities_extraction():\n",
    "    global language\n",
    "\n",
    "    def convert_to_dict(strings_list):\n",
    "        result_dict = {}\n",
    "        for string in strings_list:\n",
    "            if string in result_dict:\n",
    "                result_dict[string] += 1\n",
    "            else:\n",
    "                result_dict[string] = 1\n",
    "        return result_dict\n",
    "\n",
    "    if not Doc.has_extension(\"cvalues\"):\n",
    "        Doc.set_extension(\"cvalues\", default=None)\n",
    "\n",
    "    if language == 'fr':\n",
    "        nlp_for_ngrams = spacy.load('fr_dep_news_trf')\n",
    "    elif language == 'en':\n",
    "        nlp_for_ngrams = spacy.load('en_core_web_trf')\n",
    "    elif language == 'es':\n",
    "        nlp_for_ngrams = spacy.load('es_dep_news_trf')\n",
    "    elif language == 'de':\n",
    "        nlp_for_ngrams = spacy.load('de_dep_news_trf')\n",
    "    elif language == 'ca':\n",
    "        nlp_for_ngrams = spacy.load('ca_core_news_trf')\n",
    "    elif language == 'zh':\n",
    "        nlp_for_ngrams = spacy.load('zh_core_web_trf')\n",
    "    elif language == 'da':\n",
    "        nlp_for_ngrams = spacy.load('da_core_news_trf')\n",
    "    elif language == 'ja':\n",
    "        nlp_for_ngrams = spacy.load('ja_core_news_trf')\n",
    "    elif language == 'sl':\n",
    "        nlp_for_ngrams = spacy.load('sl_core_news_trf')\n",
    "    elif language == 'uk':\n",
    "        nlp_for_ngrams = spacy.load('uk_core_news_trf')\n",
    "\n",
    "    nlp_for_ngrams.disable_pipes('lemmatizer')\n",
    "\n",
    "    nlp_for_ngrams.add_pipe(\"cvalues\")\n",
    "\n",
    "    all_terms = {}\n",
    "    all_entities = {}\n",
    "    for num_topic in tqdm(relevant_i_by_topics, bar_format=\"{l_bar}{bar:10}{r_bar}\", desc=\"TOPICS CONFIGURATIONS PROCESSED\"):\n",
    "        all_terms[num_topic] = {}\n",
    "        all_entities[num_topic] = {}\n",
    "        for topic in relevant_i_by_topics[num_topic]:\n",
    "            ground_text = ''\n",
    "            entities_dict = {}\n",
    "            for i in relevant_i_by_topics[num_topic][topic]:\n",
    "                ground_text = ground_text + documents_lemmatized[i] + ' . '\n",
    "                actual_entities_dict = convert_to_dict(extract_np_sequences(all_tab_pos[i]))\n",
    "                for ent in actual_entities_dict:\n",
    "                    if len(ent) > 1:\n",
    "                        if ent in entities_dict:\n",
    "                            entities_dict[ent] += actual_entities_dict[ent]\n",
    "                        else:\n",
    "                            entities_dict[ent] = actual_entities_dict[ent]\n",
    "\n",
    "            obj_nlp = nlp_for_ngrams(ground_text)\n",
    "            terms = extract_terms([obj_nlp], 10)\n",
    "\n",
    "            all_terms[num_topic][topic] = terms\n",
    "            all_entities[num_topic][topic] = entities_dict\n",
    "\n",
    "    all_true_entities = {}\n",
    "    for num_topic in all_entities:\n",
    "        all_true_entities[num_topic] = {}\n",
    "        for topic in all_entities[num_topic]:\n",
    "            all_true_entities[num_topic][topic] = []\n",
    "            for ent in all_entities[num_topic][topic]:\n",
    "                actual_sum = 0\n",
    "                for topic2 in all_entities[num_topic]:\n",
    "                    if topic2 != topic:\n",
    "                        for ent2 in all_entities[num_topic][topic2]:\n",
    "                            if ent2 == ent:\n",
    "                                actual_sum += all_entities[num_topic][topic2][ent2]\n",
    "\n",
    "                if actual_sum > 0:\n",
    "                    all_true_entities[num_topic][topic].append([ent, all_entities[num_topic][topic][ent]/sqrt(actual_sum)])\n",
    "                else:\n",
    "                    all_true_entities[num_topic][topic].append([ent, all_entities[num_topic][topic][ent]])\n",
    "\n",
    "    save_to_disk_terms_or_entities(all_terms, 'terms')\n",
    "    save_to_disk_terms_or_entities(all_true_entities, 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uinIyp3VzYDO"
   },
   "outputs": [],
   "source": [
    "def extract_np_sequences(atb):\n",
    "   # unigrams = {}\n",
    "#all_false_candidates_for_unigrams = {}\n",
    "\n",
    "    pre_all_added_ngrams = []\n",
    "    ngram = []\n",
    "    det_flag = False  # Un drapeau pour vérifier si nous avons rencontré un 'det'\n",
    "\n",
    "    for pair in atb:\n",
    "        token, func = pair\n",
    "\n",
    "        # Si la fonction est 'np', ajouter le token à la séquence\n",
    "        if token in unigrams_proper_nouns:\n",
    "            if det_flag and len(ngram) > 0:\n",
    "                ngram.append(det_token)\n",
    "                det_flag = False\n",
    "\n",
    "            ngram.append(token)\n",
    "\n",
    "        # Si la fonction est 'det' et qu'il y a déjà un 'np' dans la séquence\n",
    "        elif func == 'det' and len(ngram) > 0:\n",
    "            if not det_flag:\n",
    "                det_token = token\n",
    "                det_flag = True\n",
    "            else:\n",
    "                # Si un deuxième 'det' est trouvé, vérifiez la validité de la séquence avant de la réinitialiser\n",
    "                if len(ngram) >= 1:\n",
    "                    pre_all_added_ngrams.append(' '.join(ngram))\n",
    "                ngram = []\n",
    "                det_flag = False\n",
    "\n",
    "        # Si nous rencontrons une fonction autre que 'np' ou 'det', vérifiez la validité de la séquence\n",
    "        else:\n",
    "            if len(ngram) >= 1:\n",
    "                pre_all_added_ngrams.append(' '.join(ngram))\n",
    "\n",
    "            ngram = []\n",
    "            det_flag = False\n",
    "\n",
    "    # Pour les suites en fin de tableau\n",
    "    if len(ngram) >= 1:\n",
    "        pre_all_added_ngrams.append(' '.join(ngram))\n",
    "        ngram = []\n",
    "\n",
    "    return pre_all_added_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DSS7OuozYDO"
   },
   "outputs": [],
   "source": [
    "def save_to_disk_terms_or_entities(main_dict, kind):\n",
    "    global topic_model_terms_output_name\n",
    "\n",
    "    main_dict = dict(sorted(main_dict.items()))\n",
    "\n",
    "    # Parcourez chaque sous-dictionnaire\n",
    "    for sub_dict_name, sub_dict in main_dict.items():\n",
    "\n",
    "        sub_dict = dict(sorted(sub_dict.items()))\n",
    "\n",
    "        for key, value in sorted(sub_dict.items()):\n",
    "            sorted_value = sorted(value, key=lambda x: x[1], reverse=True)\n",
    "            sub_dict[key] = sorted_value[:10]\n",
    "\n",
    "        # Créez un chemin pour le fichier correspondant au sous-dictionnaire\n",
    "        if kind == 'entities':\n",
    "            file_path = results_path + base_name + f\"_topic_model_entities_output_name_{sub_dict_name}t.csv\"\n",
    "        else:\n",
    "            file_path = results_path + base_name + f\"_topic_model_terms_output_name_{sub_dict_name}t.csv\"\n",
    "\n",
    "        # Ouvrez le fichier pour écrire\n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Créez les en-têtes basés sur les clés du sous-dictionnaire\n",
    "            headers = []\n",
    "            for key in sub_dict.keys():\n",
    "                if kind == 'entities':\n",
    "                    headers.extend([f\"{key}_entities\", f\"{key}_scores\"])\n",
    "                else:\n",
    "                    headers.extend([f\"{key}_terms\", f\"{key}_scores\"])\n",
    "\n",
    "            writer.writerow(headers)\n",
    "\n",
    "            # Trouvez le tableau le plus long pour déterminer le nombre de lignes\n",
    "            max_length = max(len(value) for value in sub_dict.values())\n",
    "\n",
    "            # Écrivez les données\n",
    "            for i in range(max_length):\n",
    "                row = []\n",
    "                for key, value in sub_dict.items():\n",
    "                    if i < len(value):\n",
    "                        row.extend(value[i])\n",
    "                    else:\n",
    "                        row.extend([None, None])  # Ajoutez des valeurs vides si le tableau est plus court\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP8von2jzYDO"
   },
   "outputs": [],
   "source": [
    "def create_topic_barplots():\n",
    "    global original_labels\n",
    "    dict_final = {}\n",
    "\n",
    "    for key, sub_dict in every_topics_and_scores_by_document.items():\n",
    "        new_dict = {}\n",
    "        sum_dict = {}\n",
    "\n",
    "        # Calculez la somme et le nombre d'occurrences pour chaque élément\n",
    "        for tuple_list in sub_dict.values():\n",
    "            for t in tuple_list:\n",
    "                if t[0] in new_dict:\n",
    "                    new_dict[t[0]].append(t[1])\n",
    "                else:\n",
    "                    new_dict[t[0]] = [t[1]]\n",
    "\n",
    "        # Calculer la somme et la variance pour chaque élément\n",
    "        for topic, values in new_dict.items():\n",
    "            n = len(values)\n",
    "            mean = sum(values) / n\n",
    "            variance = sum((x - mean) ** 2 for x in values) / n\n",
    "            sum_dict[topic] = (sum(values), variance)\n",
    "\n",
    "        dict_final[key] = sum_dict\n",
    "\n",
    "    sns.set_style(\"white\")  # Fond blanc, pas de grille\n",
    "\n",
    "    variances = {}\n",
    "    y_values = {}\n",
    "\n",
    "    cmap = plt.cm.coolwarm\n",
    "\n",
    "    for key in dict_final:\n",
    "        # Trier le sous-dictionnaire basé sur la première valeur du tuple\n",
    "        sorted_items = sorted(dict_final[key].items(), key=lambda x: x[1][0])\n",
    "        sorted_keys = [k for k, _ in sorted_items]\n",
    "\n",
    "        # Récupérer les chaînes correspondantes depuis original_labels\n",
    "        sorted_strings = [original_labels[key][k] for k in sorted_keys]\n",
    "        sorted_strings = sorted_strings[::-1]\n",
    "\n",
    "        # Extraire les variances en suivant l'ordre des clés triées\n",
    "        sorted_variances = [dict_final[key][k][1] for k in sorted_keys]\n",
    "        sorted_variances = sorted_variances[::-1]\n",
    "\n",
    "        variances[key] = sorted_variances\n",
    "\n",
    "        sorted_first_values = [dict_final[key][k][0] for k in sorted_keys]\n",
    "        sorted_first_values = sorted_first_values[::-1]\n",
    "        y_values[key] = sorted_first_values\n",
    "\n",
    "        # Afficher les résultats\n",
    "        print(f\"Clé {key}: Chaînes triées: {sorted_strings}\")\n",
    "        print(f\"Clé {key}: Variances triées: {variances[key]}\")\n",
    "        print(f\"Clé {key}: Y triées: {y_values[key]}\")\n",
    "\n",
    "        # Normaliser les variances pour obtenir des valeurs entre 0 et 1\n",
    "        normed_variances = np.interp(variances[key], (min(variances[key]), max(variances[key])), (0, 1))\n",
    "\n",
    "        # Convertir les variances normalisées en couleurs\n",
    "        colors = [cmap(v) for v in normed_variances]\n",
    "\n",
    "        num_bars = len(sorted_strings)\n",
    "        height_per_bar = 0.5\n",
    "        total_height = num_bars * height_per_bar\n",
    "\n",
    "        plt.figure(figsize=(4, total_height), dpi=150)\n",
    "\n",
    "        sns.barplot(y=np.array(sorted_strings),\n",
    "                    x=np.array(y_values[key]),\n",
    "                    hue=np.array(sorted_strings),\n",
    "                    palette=colors,\n",
    "                    orient='h',\n",
    "                    dodge=False)\n",
    "\n",
    "        plt.legend([],[], frameon=False)  # Cela cache la légende\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.box(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{results_path}{base_name}_{key}t_barplot.png\", format='png', dpi=300, bbox_inches='tight')  # Sauvegarde le plot\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1WOhwx-zYDO"
   },
   "outputs": [],
   "source": [
    "def create_umap_best_ngrams(unigrams_nb, n_neighbors):\n",
    "    global unigrams_common_nouns\n",
    "\n",
    "    _, tfidf, tfidf_feature_names, _ = go_tfidf_vectorization(unigrams_common_nouns)\n",
    "\n",
    "    sum_tfidf = tfidf.sum(axis=0)\n",
    "    average_tfidf = sum_tfidf / tfidf.shape[0]\n",
    "    average_tfidf = np.array(average_tfidf).flatten()\n",
    "    sorted_indices = average_tfidf.argsort()[::-1]\n",
    "    top_50_indices = sorted_indices[:unigrams_nb]\n",
    "    X_reduced = tfidf[:, top_50_indices]\n",
    "    similarity_matrix = cosine_similarity(X_reduced.T)\n",
    "    embedding = umap.UMAP(n_neighbors=n_neighbors).fit_transform(similarity_matrix)\n",
    "\n",
    "    std_devs = np.std(tfidf.toarray(), axis=0)\n",
    "    color_values_array = [-1 for _ in range(embedding.shape[0])]\n",
    "\n",
    "    nb = 0\n",
    "    for idx in top_50_indices:\n",
    "        color_values_array[nb] = std_devs[idx]\n",
    "        nb += 1\n",
    "\n",
    "    min_value = np.min(color_values_array)\n",
    "    sizes = 5 * (color_values_array / min_value)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], marker=\"\", legend=False)\n",
    "\n",
    "    norm = plt.Normalize(np.min(color_values_array), np.max(color_values_array))\n",
    "    colors = plt.cm.winter(norm(color_values_array))\n",
    "\n",
    "    texts = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for i, multigram in enumerate(tfidf_feature_names[top_50_indices]):\n",
    "        lemma = (multigram)\n",
    "        texts.append(plt.text(embedding[i, 0], embedding[i, 1], lemma, size=8, c=colors[i], ha='center', va='center'))\n",
    "\n",
    "\n",
    "    adjust_text(texts)\n",
    "\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "    plt.gca().xaxis.set_ticks([])\n",
    "    plt.gca().yaxis.set_ticks([])\n",
    "\n",
    "    all_std_by_name = {}\n",
    "    for idx in top_50_indices:\n",
    "        all_std_by_name[tfidf_feature_names[idx]] = std_devs[idx]\n",
    "\n",
    "    with open(f\"{results_path}{base_name}_main_common_nouns_tfidf_std.csv\", \"w\", newline='', encoding='utf-8') as fichier_csv:\n",
    "        writer = csv.writer(fichier_csv)\n",
    "\n",
    "        # Parcourir le dictionnaire et écrire chaque paire clé-valeur\n",
    "        for cle, valeur in all_std_by_name.items():\n",
    "            writer.writerow([cle, valeur])\n",
    "\n",
    "    plt.savefig(f\"{results_path}{base_name}_umap_{unigrams_nb}un_{n_neighbors}nn_network.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__sFymq2zYDO"
   },
   "outputs": [],
   "source": [
    "def determine_unigrams():\n",
    "    global unigrams\n",
    "    global unigrams_common_nouns\n",
    "    global unigrams_nouns\n",
    "    global unigrams_proper_nouns\n",
    "\n",
    "    unigrams = update_candidates_for_unigram('np', unigrams)\n",
    "    unigrams_nouns = update_candidates_for_unigram('np', unigrams_nouns)\n",
    "    unigrams_proper_nouns = update_candidates_for_unigram('np', unigrams_proper_nouns)\n",
    "\n",
    "    unigrams = update_candidates_for_unigram('nc', unigrams)\n",
    "    unigrams_nouns = update_candidates_for_unigram('nc', unigrams_nouns)\n",
    "    unigrams_common_nouns = update_candidates_for_unigram('nc', unigrams_common_nouns)\n",
    "\n",
    "    unigrams = update_candidates_for_unigram('v', unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAw3i_UqzYDO"
   },
   "outputs": [],
   "source": [
    "def write_raw_documents():\n",
    "  with open(raw_documents_output_name, \"w\", encoding='utf-8') as file_object:\n",
    "    for dfn in documents:\n",
    "      file_object.write(dfn + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6QrgrEG_2zh"
   },
   "outputs": [],
   "source": [
    "def write_lemmatized_documents():\n",
    "  with open(lemmatized_documents_output_name, \"w\", encoding='utf-8') as file_object:\n",
    "    for dfn in documents_lemmatized:\n",
    "      file_object.write(dfn + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3uQkeWz_212"
   },
   "outputs": [],
   "source": [
    "def extract_date_from_text(date_text):\n",
    "    # Vérifier si le texte est déjà au format 'XX/XX/XXXX'\n",
    "\n",
    "    if language == 'fr':\n",
    "        if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_text):\n",
    "            return date_text\n",
    "\n",
    "        # Si ce n'est pas le cas, essayer de le convertir\n",
    "        pattern = r\"(\\d{1,2})\\s+([a-zA-Zéû]+)\\s+(\\d{4})\"\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "\n",
    "            # Correction des éventuelles typos ou mots manquants dans les mois\n",
    "            if month.lower() == \"aot\":\n",
    "                month = \"août\"\n",
    "\n",
    "            month_dict = {\n",
    "                'janvier': '01', 'février': '02', 'mars': '03', 'avril': '04', 'mai': '05',\n",
    "                'juin': '06', 'juillet': '07', 'août': '08', 'septembre': '09', 'octobre': '10',\n",
    "                'novembre': '11', 'décembre': '12'\n",
    "            }\n",
    "\n",
    "            month_num = month_dict.get(month.lower())\n",
    "            if month_num:\n",
    "                return f\"{day}/{month_num}/{year}\"\n",
    "    elif language == 'en':\n",
    "        if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_text):\n",
    "            return date_text\n",
    "\n",
    "        pattern = r\"(\\d{1,2})\\s+([a-zA-Z]+)\\s+(\\d{4})\"\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "\n",
    "            month_dict = {\n",
    "                'january': '01', 'february': '02', 'march': '03', 'april': '04', 'may': '05',\n",
    "                'june': '06', 'july': '07', 'august': '08', 'september': '09', 'october': '10',\n",
    "                'november': '11', 'december': '12'\n",
    "            }\n",
    "\n",
    "            month_num = month_dict.get(month.lower())\n",
    "            if month_num:\n",
    "                return f\"{day}/{month_num}/{year}\"\n",
    "\n",
    "    elif language == 'pt':\n",
    "        if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_text):\n",
    "            return date_text\n",
    "\n",
    "        # Modèle pour le format \"DD Month YYYY\"\n",
    "        pattern = r\"(\\d{1,2})\\s+([a-zA-Zçûá]+)\\s+(\\d{4})\"\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "\n",
    "            # Dictionnaire des mois en portugais\n",
    "            month_dict = {\n",
    "                'janeiro': '01', 'fevereiro': '02', 'março': '03', 'abril': '04', 'maio': '05',\n",
    "                'junho': '06', 'julho': '07', 'agosto': '08', 'setembro': '09', 'outubro': '10',\n",
    "                'novembro': '11', 'dezembro': '12'\n",
    "            }\n",
    "\n",
    "            month_num = month_dict.get(month.lower())\n",
    "            if month_num:\n",
    "                return f\"{day}/{month_num}/{year}\"\n",
    "\n",
    "    elif language == 'jp':\n",
    "        # Modèle pour le format japonais \"YYYY年MM月DD日\"\n",
    "        pattern = r\"(\\d{1,4})年(\\d{1,2})月(\\d{1,2})日\"\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            year, month, day = match.groups()\n",
    "            return f\"{year}/{month.zfill(2)}/{day.zfill(2)}\"\n",
    "\n",
    "    elif language == 'es':\n",
    "        if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_text):\n",
    "            return date_text\n",
    "\n",
    "        # Modèle pour le format \"DD Month YYYY\"\n",
    "        pattern = r\"(\\d{1,2})\\s+([a-zA-Z]+)\\s+(\\d{4})\"\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "\n",
    "            # Dictionnaire des mois en espagnol\n",
    "            month_dict = {\n",
    "                'enero': '01', 'febrero': '02', 'marzo': '03', 'abril': '04', 'mayo': '05',\n",
    "                'junio': '06', 'julio': '07', 'agosto': '08', 'septiembre': '09', 'octubre': '10',\n",
    "                'noviembre': '11', 'diciembre': '12'\n",
    "            }\n",
    "\n",
    "            month_num = month_dict.get(month.lower())\n",
    "            if month_num:\n",
    "                return f\"{day}/{month_num}/{year}\"\n",
    "\n",
    "\n",
    "    return date_text  # Retourner le texte original s'il ne peut pas être converti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l4UeX3yzYDO"
   },
   "outputs": [],
   "source": [
    "def extract_information(header, selector):\n",
    "    elements = header.select(selector)\n",
    "    if elements:\n",
    "        return \"////\".join([get_text_from_tag(el).replace(';', ',') for el in elements])\n",
    "    else:\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikKezsdQzYDO"
   },
   "outputs": [],
   "source": [
    "def get_text_from_tag(tag):\n",
    "    return ''.join(tag.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oryU-8PozYDO"
   },
   "outputs": [],
   "source": [
    "def normalize_journal(t):\n",
    "    t = t.strip()\n",
    "\n",
    "    # Supprimer tout ce qui se trouve après la première virgule\n",
    "    t = re.sub(r',.*', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui se trouve après le premier tiret\n",
    "    t = re.sub(r'-.*', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui se trouve entre parenthèses (y compris les parenthèses)\n",
    "    t = re.sub(r'\\(.*?\\)', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui suit trois espaces vides ou plus\n",
    "    t = re.sub(r' {3,}.*', '', t)\n",
    "\n",
    "    # Trim le texte\n",
    "    t = t.strip()\n",
    "\n",
    "    return t.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEWRP5HBzYDO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_date_info(date_text, language='fr'):\n",
    "    if language == 'fr':\n",
    "        regex = \"([1-3]?[0-9]\\\\s(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\\\s20[0-2][0-9])\"\n",
    "    elif language == 'en':\n",
    "        regex = \"([1-3]?[0-9]\\\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s20[0-2][0-9])\"\n",
    "\n",
    "    date_text_clean = re.search(regex, date_text)\n",
    "    return date_text_clean.group() if date_text_clean else date_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2u6MilUzYDO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalise_date(date_text):\n",
    "    global language\n",
    "\n",
    "    # Dictionnaires pour la conversion des mois\n",
    "    month_dict = {\n",
    "        'en': {\n",
    "            'january': '01', 'february': '02', 'march': '03', 'april': '04', 'may': '05',\n",
    "            'june': '06', 'july': '07', 'august': '08', 'september': '09', 'october': '10',\n",
    "            'november': '11', 'december': '12'\n",
    "        },\n",
    "        'fr': {\n",
    "            'janvier': '01', 'février': '02', 'mars': '03', 'avril': '04', 'mai': '05',\n",
    "            'juin': '06', 'juillet': '07', 'août': '08', 'septembre': '09', 'octobre': '10',\n",
    "            'novembre': '11', 'décembre': '12'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Regex pour extraire la date, adaptée à l'exemple fourni\n",
    "    pattern = r\"([a-zA-Z]+)\\s+(\\d{1,2}),\\s+(\\d{4})\"\n",
    "    match = re.search(pattern, date_text)\n",
    "\n",
    "    if match:\n",
    "        month, day, year = match.groups()\n",
    "        month = month.lower()\n",
    "\n",
    "        # Convertir le mois en chiffre en utilisant le dictionnaire\n",
    "        if month in month_dict[language]:\n",
    "            month_num = month_dict[language][month]\n",
    "            return f\"{day}/{month_num}/{year}\"\n",
    "\n",
    "    # Retourner le texte original si aucun motif n'est trouvé\n",
    "    return date_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kkopFTdzYDP"
   },
   "outputs": [],
   "source": [
    "def standardize_name(name):\n",
    "    words = name.split()\n",
    "    words.sort()\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKrisLVXzYDP"
   },
   "outputs": [],
   "source": [
    "def split_names(s):\n",
    "    words = s.split()\n",
    "    if len(words) == 4:\n",
    "        first_name = ' '.join(words[:2])\n",
    "        second_name = ' '.join(words[2:])\n",
    "        return [first_name, second_name]\n",
    "    elif len(words) == 6:\n",
    "        first_name = ' '.join(words[0:2])\n",
    "        second_name = ' '.join(words[2:4])\n",
    "        third_name = ' '.join(words[4:6])\n",
    "        return [first_name, second_name, third_name]\n",
    "    elif len(words) == 8:\n",
    "        first_name = ' '.join(words[0:2])\n",
    "        second_name = ' '.join(words[2:4])\n",
    "        third_name = ' '.join(words[4:6])\n",
    "        fourth_name = ' '.join(words[4:6])\n",
    "        return [first_name, second_name, third_name, fourth_name]\n",
    "\n",
    "    return [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjo8OSp5zYDP"
   },
   "outputs": [],
   "source": [
    "def extract_names(line):\n",
    "    if len(line) > 150:\n",
    "        return None\n",
    "\n",
    "    # Supprimer tout ce qui est entre parenthèses\n",
    "    line = re.sub(r'\\(.*?\\)', '', line)\n",
    "\n",
    "    # Ignorer les lignes qui contiennent des domaines ou \"N/A\"\n",
    "    if re.search(r'(\\.fr|\\.com|n/a)', line):\n",
    "        return None\n",
    "\n",
    "    # Supprimer les occurrences de \"Envoyé[e] spécial[e] à X\"\n",
    "    line = re.sub(r'envoyé[e]? spécial[e]? à \\w+', '', line)\n",
    "\n",
    "    line = re.sub(r'correspondant[e]? à \\w+', '', line)\n",
    "    line = re.sub(r'correspondant[e]? en \\w+', '', line)\n",
    "    line = re.sub(r'recueilli par', '', line)\n",
    "\n",
    "    line = line.replace('rédacteur en chef à', '')\n",
    "    line = line.replace('rédactrice en chef à', '')\n",
    "\n",
    "    line = line.replace('correspondant européen', '')\n",
    "    line = line.replace('correspondant étranger', '')\n",
    "\n",
    "    line = line.replace(\"la rédaction de l'agence\", '')\n",
    "\n",
    "    line = re.sub(r'\\s?@\\w+', '', line)\n",
    "\n",
    "    line = line.replace('par', '')\n",
    "    line = line.replace('.', '')\n",
    "\n",
    "    line = line.replace('\"', '')\n",
    "    line = line.replace('«', '')\n",
    "    line = line.replace('»', '')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "\n",
    "    line = line.replace('directeur du monde', '')\n",
    "    line = line.replace('propos recueillis', '')\n",
    "    line = line.replace('cofondateur de libération', '')\n",
    "    line = line.replace('e. gr.', '')\n",
    "    line = line.replace('les echos', '')\n",
    "    line = line.replace('repères', '')\n",
    "    line = line.replace(\"l'air du large\", '')\n",
    "    line = line.replace('interview', '')\n",
    "    line = line.replace(\"professeur agrégé d'histoire à sciences po, docteur en géographie\", '')\n",
    "\n",
    "\n",
    "    line = line.replace('la croix', '')\n",
    "\n",
    "    line = line.replace('york', '')\n",
    "    line = line.replace('avec', '')\n",
    "\n",
    "    # Si la ligne contient \"////\", supprimez tout ce qui est à gauche et \"////\" lui-même\n",
    "    if \"////\" in line:\n",
    "        line = line.split(\"////\")[1].strip()\n",
    "\n",
    "    line = line.replace(',', ', ')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "\n",
    "    # Si la ligne contient des virgules ou \"et\", divisez la ligne et prenez les noms\n",
    "\n",
    "    names = []\n",
    "    if len(line.split()) > 3:\n",
    "        parts = re.split(',| et', line)\n",
    "        for part in parts:\n",
    "            names.extend(split_names(part.strip()))\n",
    "    else:\n",
    "        line = line.replace(',', '')\n",
    "        names.extend(split_names(line.strip()))\n",
    "\n",
    "    return set(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gduzePvmQahL"
   },
   "outputs": [],
   "source": [
    "def write_info_europresse_lemmas(lemmas_dict, topic_nums, article, actual_doc):\n",
    "    all_lemmas = []\n",
    "\n",
    "    header = article.header\n",
    "\n",
    "    title_text = extract_information(header, '.titreArticle p')\n",
    "    journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "    date_text = extract_information(header, '.DocHeader')\n",
    "\n",
    "    journal_text = normalize_journal(journal_text)\n",
    "    date_text_clean = extract_date_info(date_text)\n",
    "    date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "    date_normalized = extract_date_from_text(date_normalized)\n",
    "\n",
    "    formatted_list = [\"{};{}\".format(topic, score) for topic, score in topic_nums]\n",
    "    topics_str = \";\".join(formatted_list)\n",
    "\n",
    "    all_names = None\n",
    "    names = extract_names(extract_information(header, '.sm-margin-bottomNews').lower())\n",
    "    if names:\n",
    "        actual_names = [standardize_name(name) for name in names]\n",
    "        filtered_names = [name for name in actual_names if not any(other_name != name and set(name.split()) < set(other_name.split()) for other_name in actual_names)]\n",
    "        all_names = filtered_names\n",
    "\n",
    "    if all_names is None:\n",
    "        chaine_authors = \"None\"\n",
    "    else:\n",
    "        chaine_authors = ', '.join(map(str, all_names))\n",
    "\n",
    "    for lemmapos in lemmas_dict:\n",
    "        all_lemmas.append(f\"{lemmapos[0]};{lemmapos[1]};{lemmas_dict[lemmapos]};{title_text.replace(';', '')};{chaine_authors};{extract_information(header, '.sm-margin-bottomNews').lower()};{len(actual_doc)};{journal_text.replace(';', '')};{date_normalized};{topics_str}\")\n",
    "\n",
    "    return all_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMywMyATQahL"
   },
   "outputs": [],
   "source": [
    "def write_info_europresse(topic_nums, article, actual_doc):\n",
    "    header = article.header\n",
    "\n",
    "    title_text = extract_information(header, '.titreArticle p')\n",
    "    journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "    date_text = extract_information(header, '.DocHeader')\n",
    "\n",
    "    journal_text = normalize_journal(journal_text)\n",
    "    date_text_clean = extract_date_info(date_text)\n",
    "    date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "    date_normalized = extract_date_from_text(date_normalized)\n",
    "\n",
    "    formatted_list = [\"{};{}\".format(topic, score) for topic, score in topic_nums]\n",
    "    topics_str = \";\".join(formatted_list)\n",
    "\n",
    "    all_names = None\n",
    "    names = extract_names(extract_information(header, '.sm-margin-bottomNews').lower())\n",
    "    if names:\n",
    "        actual_names = [standardize_name(name) for name in names]\n",
    "        filtered_names = [name for name in actual_names if not any(other_name != name and set(name.split()) < set(other_name.split()) for other_name in actual_names)]\n",
    "        all_names = filtered_names\n",
    "\n",
    "    if all_names is None:\n",
    "        chaine_authors = \"None\"\n",
    "    else:\n",
    "        chaine_authors = ', '.join(map(str, all_names))\n",
    "\n",
    "    return f\"{title_text.replace(';', '')};{chaine_authors};{extract_information(header, '.sm-margin-bottomNews').lower()};{len(actual_doc)};{journal_text.replace(';', '')};{date_normalized};{topics_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMc5J9tR_24d"
   },
   "outputs": [],
   "source": [
    "def write_info_another(topic_nums, columns_dicts, i, actual_doc):\n",
    "\n",
    "    formatted_list = [\"{};{}\".format(topic, score) for topic, score in topic_nums]\n",
    "    topics_str = \";\".join(formatted_list)\n",
    "\n",
    "    row = ''\n",
    "    for key_title in columns_dicts:\n",
    "        row += str(columns_dicts[key_title][i]) + ';'\n",
    "\n",
    "    return f\"{row}{len(actual_doc)};{topics_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfLc3z4qQahL"
   },
   "outputs": [],
   "source": [
    "def write_info_another_lemmas(lemmas_dict, topic_nums, columns_dicts, i, actual_doc):\n",
    "    all_lemmas = []\n",
    "\n",
    "    formatted_list = [\"{};{}\".format(topic, score) for topic, score in topic_nums]\n",
    "    topics_str = \";\".join(formatted_list)\n",
    "\n",
    "    row = ''\n",
    "    for key_title in columns_dicts:\n",
    "        row += str(columns_dicts[key_title][i]) + ';'\n",
    "\n",
    "    for lemmapos in lemmas_dict:\n",
    "        all_lemmas.append(f\"{lemmapos[0]};{lemmapos[1]};{lemmas_dict[lemmapos]};{row}{len(actual_doc)};{topics_str}\")\n",
    "\n",
    "    return all_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p889n3Ya_26p"
   },
   "outputs": [],
   "source": [
    "def lister_html_insensible(dossier):\n",
    "    return [f for f in os.listdir(dossier) if f.lower().endswith('.html') and os.path.isfile(os.path.join(dossier, f)) and base_name in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP8r-C6EzYDP"
   },
   "outputs": [],
   "source": [
    "def remove_urls_hashtags_emojis_mentions_emails(text):\n",
    "    # Supprimer les URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "    # Supprimer les hashtags\n",
    "   # text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Supprimer les mentions\n",
    "  #  text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Supprimer les e-mails\n",
    " #   text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "    # Supprimer les émojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    " #   text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQVOHqZFzYDP"
   },
   "outputs": [],
   "source": [
    "def write_lemmatized_documents():\n",
    "    with open(lemmatized_documents_output_name, \"w\", encoding='utf-8') as file_object:\n",
    "        for d in documents_lemmatized:\n",
    "            file_object.write(d + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjTq-_ZEzYDQ"
   },
   "outputs": [],
   "source": [
    "def extract_info(topic_nums, article):\n",
    "    header = article.header\n",
    "\n",
    "    date_text = extract_information(header, '.DocHeader')\n",
    "    date_text_clean = extract_date_info(date_text)\n",
    "    date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "    date_normalized = extract_date_from_text(date_normalized)\n",
    "\n",
    "    topics_dict = dict(topic_nums)\n",
    "\n",
    "    return {date_normalized: topics_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vB-tYyThzYDT"
   },
   "outputs": [],
   "source": [
    "def aggregate_scores(articles_info):\n",
    "    aggregated_scores = {}\n",
    "\n",
    "    for info in articles_info:\n",
    "        for date, topics in info.items():\n",
    "            if date not in aggregated_scores:\n",
    "                aggregated_scores[date] = {}\n",
    "\n",
    "            for topic, score in topics.items():\n",
    "                if topic not in aggregated_scores[date]:\n",
    "                    aggregated_scores[date][topic] = 0\n",
    "                aggregated_scores[date][topic] += score\n",
    "\n",
    "    return aggregated_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wvS55ERzYDU"
   },
   "outputs": [],
   "source": [
    "def count_articles_by_date(articles_info):\n",
    "    article_counts = {}\n",
    "\n",
    "    for info in articles_info:\n",
    "        for date in info.keys():\n",
    "            if date not in article_counts:\n",
    "                article_counts[date] = 0\n",
    "            article_counts[date] += 1\n",
    "\n",
    "    return article_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMX1VVFPzYDU"
   },
   "outputs": [],
   "source": [
    "def normalize_scores_by_article_count(aggregated_scores, aggregated_article_counts):\n",
    "    normalized_scores = {}\n",
    "\n",
    "    for date, topics in aggregated_scores.items():\n",
    "        if date not in normalized_scores:\n",
    "            normalized_scores[date] = {}\n",
    "\n",
    "        for topic, score in topics.items():\n",
    "            normalized_scores[date][topic] = score / aggregated_article_counts[date]\n",
    "\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvBhn9Z2zYDU"
   },
   "outputs": [],
   "source": [
    "def create_chrono_topics(normalize_by_date,\n",
    "                         relative_normalizaton,\n",
    "                         sigma):\n",
    "    global all_nmf_W\n",
    "    global original_labels\n",
    "    global every_topics_and_scores_by_document\n",
    "    global is_europresse\n",
    "    global columns_dicts\n",
    "\n",
    "    from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "\n",
    "    for num_topic in tqdm(all_nmf_W, bar_format=\"{l_bar}{bar:10}{r_bar}\", desc=\"TOPICS CONFIGURATIONS PROCESSED\"):\n",
    "        if is_europresse:\n",
    "            articles_info = [extract_info(every_topics_and_scores_by_document[num_topic][i], article) for i, article in enumerate(all_soups)]\n",
    "        else:\n",
    "            from dateutil.parser import parse\n",
    "\n",
    "            def detecter_date(chaine, jour_en_premier=True):\n",
    "                try:\n",
    "                    return parse(chaine, dayfirst=jour_en_premier)\n",
    "                except ValueError:\n",
    "                    return None\n",
    "\n",
    "            def formater_date(date):\n",
    "                return date.strftime('%d/%m/%Y')\n",
    "\n",
    "            def formater_liste_dates(liste_dates, jour_en_premier=True):\n",
    "                return [formater_date(detecter_date(date_str, jour_en_premier)) for date_str in liste_dates if detecter_date(date_str, jour_en_premier)]\n",
    "\n",
    "            articles_info = []\n",
    "            all_dates = formater_liste_dates(columns_dicts['date'])\n",
    "            i = 0\n",
    "            while i < len(all_dates):\n",
    "                articles_info.append({all_dates[i]: dict(every_topics_and_scores_by_document[num_topic][i])})\n",
    "                i += 1\n",
    "\n",
    "        aggregated_scores = aggregate_scores(articles_info)\n",
    "\n",
    "        def extract_and_convert_date(date_str):\n",
    "            # Utilisation d'une expression régulière pour extraire la date\n",
    "            match = re.search(r'\\d{2}/\\d{2}/\\d{4}', date_str)\n",
    "            if match:\n",
    "                return datetime.strptime(match.group(), '%d/%m/%Y')\n",
    "            else:\n",
    "                # Gérer les cas où aucune date n'est trouvée\n",
    "                return None\n",
    "\n",
    "        # Filtrer et convertir les dates\n",
    "        valid_dates = {}\n",
    "        for date_str, score in aggregated_scores.items():\n",
    "            date = extract_and_convert_date(date_str)\n",
    "            if date:\n",
    "                valid_dates[date.strftime('%d/%m/%Y')] = score\n",
    "\n",
    "        # Trier les dates valides\n",
    "        # sorted_dates = sorted(valid_dates.keys(), key=lambda date: datetime.strptime(date, '%d/%m/%Y'))\n",
    "        aggregated_scores_sorted = {date: valid_dates[date]\n",
    "                                    for date in sorted(valid_dates,\n",
    "                                                    key=lambda date: datetime.strptime(date, '%d/%m/%Y'))}\n",
    "\n",
    "        #aggregated_scores_sorted = {date: valid_dates[date] for date in sorted_dates}\n",
    "        aggregated_article_counts = count_articles_by_date(articles_info)\n",
    "\n",
    "        with open(f\"{results_path}{base_name}_aggregated_scores_sorted_{num_topic}.json\", 'w', encoding='utf-8') as file:\n",
    "            json.dump(aggregated_scores_sorted, file, indent=4)\n",
    "\n",
    "        if normalize_by_date:\n",
    "            aggregated_scores_sorted = normalize_scores_by_article_count(aggregated_scores_sorted, aggregated_article_counts)\n",
    "\n",
    "            with open(f\"{results_path}{base_name}_aggregated_scores_sorted_normalized_by_date_{num_topic}.json\",\n",
    "                    'w',\n",
    "                    encoding='utf-8') as file:\n",
    "                json.dump(aggregated_scores_sorted, file, indent=4)\n",
    "\n",
    "        df = pd.DataFrame(aggregated_scores_sorted)\n",
    "        df_normalized = df.div(df.max(axis=1), axis=0)\n",
    "\n",
    "        # Convertir les colonnes en type datetime pour une manipulation plus aisée\n",
    "        df_normalized.columns = pd.to_datetime(df_normalized.columns, dayfirst=True)\n",
    "\n",
    "        # Convertir les clés en dates et trouver la plus éloignée dans le temps\n",
    "        dates = [datetime.strptime(date, \"%d/%m/%Y\") for date in aggregated_scores_sorted.keys()]\n",
    "        newest_date = max(dates)\n",
    "\n",
    "        newest_date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        # Convertir les clés en dates et trouver la plus éloignée dans le temps\n",
    "        dates = [datetime.strptime(date, \"%d/%m/%Y\") for date in aggregated_scores_sorted.keys()]\n",
    "        oldest_date = min(dates)\n",
    "\n",
    "        oldest_date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        date_range = pd.date_range(start=oldest_date,\n",
    "                                end=newest_date)\n",
    "\n",
    "        # Réindexer le dataframe pour inclure toutes les dates dans le range, en remplissant les valeurs manquantes par 0\n",
    "        df_normalized = df_normalized.reindex(columns=date_range, fill_value=0)\n",
    "\n",
    "        #sigma = len(df_normalized.columns)/10\n",
    "\n",
    "        list_of_series = []\n",
    "\n",
    "        for index, row in df_normalized.iterrows():\n",
    "            if sigma == 'auto':\n",
    "                filtered_values = gaussian_filter(row, sigma=(df_normalized.std(axis=1).mean()*df_normalized.std(axis=1).mean())*300)\n",
    "            else:\n",
    "                filtered_values = gaussian_filter(row, sigma=sigma)\n",
    "\n",
    "            s = pd.Series(filtered_values, index=df_normalized.columns, name=index)\n",
    "            list_of_series.append(s)\n",
    "\n",
    "        df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "        if relative_normalizaton:\n",
    "            list_of_series = []\n",
    "\n",
    "            for index, row in df_normalized.iterrows():\n",
    "                normalized_values = (row - row.min()) / (row.max() - row.min())\n",
    "                s = pd.Series(normalized_values, index=df_normalized.columns, name=index)\n",
    "                list_of_series.append(s)\n",
    "\n",
    "            df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "        dist_matrix = manhattan_distances(df_normalized.values)\n",
    "        condensed_dist_matrix = squareform(dist_matrix)\n",
    "\n",
    "        Z = linkage(condensed_dist_matrix, method='complete', optimal_ordering=True)   # method='ward')\n",
    "        dendro = dendrogram(Z, no_plot=True)\n",
    "        df_normalized = df_normalized.iloc[dendro['leaves']]\n",
    "        reordered_labels = [original_labels[num_topic][i] for i in np.array(df_normalized.index)]\n",
    "\n",
    "        #plt.figure(figsize=(20, 12))\n",
    "        fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "        ax = sns.heatmap(df_normalized, cmap=\"YlGnBu\", cbar=False)\n",
    "\n",
    "    #  ax.set_yticklabels(reordered_labels, rotation=0)\n",
    "\n",
    "        offset_x = int(len(dates) / 100)\n",
    "        offset_x = 0\n",
    "        for index, label in enumerate(reordered_labels):\n",
    "            plt.text(x=-offset_x,\n",
    "                    y=(index + 0.5),\n",
    "                    s=label,\n",
    "                    rotation=0,\n",
    "                    ha='right',\n",
    "                    va='center',\n",
    "                    fontname='Liberation Mono')\n",
    "\n",
    "\n",
    "        # Calcul de l'intervalle dynamique pour les dates\n",
    "        interval = calculate_date_interval(df_normalized.T,\n",
    "                                        figsize=(20, 12))\n",
    "\n",
    "        longest_string = max(reordered_labels, key=len)\n",
    "\n",
    "        # Étape 2: Obtenir la longueur du string le plus long\n",
    "        longest_string_length = len(longest_string)\n",
    "\n",
    "        if interval < 2:\n",
    "            interval = 2\n",
    "\n",
    "        interval = int(interval * (0.0135*longest_string_length + 0.9865))\n",
    "\n",
    "        # Configuration de l'axe des abscisses avec l'intervalle dynamique\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=interval))\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ticks = ax.get_xticks()\n",
    "\n",
    "      #  ticks = ticks[:-1]\n",
    "\n",
    "        labels = [date.strftime('%d-%m-%Y') for date in mdates.num2date(ticks)]\n",
    "\n",
    "        ax.set_xlabel('')  # Supprimer le titre de l'axe des x\n",
    "        ax.set_ylabel('')  # Supprimer le titre de l'axe des y\n",
    "        ax.set_title('')  # Supprimer le titre du graphique\n",
    "        ax.set_xticks([])  # Supprime les ticks de l'axe x\n",
    "        ax.set_yticks([])  # Supprime les ticks de l'axe y\n",
    "\n",
    "        unix_base_date = datetime(1970, 1, 1)\n",
    "\n",
    "        base_date = datetime.date(df_normalized.T.index.min())\n",
    "\n",
    "        # Convertir les labels de dates et les ajuster\n",
    "        adjusted_dates = []\n",
    "        for label in labels:\n",
    "            # Convertir le label en objet datetime\n",
    "            label_date = datetime.strptime(label, \"%d-%m-%Y\")\n",
    "\n",
    "            # Calculer la différence en jours\n",
    "            delta_days = (label_date - unix_base_date).days\n",
    "\n",
    "            # Ajouter cette différence à la date de base de votre corpus\n",
    "            new_date = base_date + timedelta(days=delta_days)\n",
    "\n",
    "            # Ajouter à la liste ajustée (formattez si nécessaire)\n",
    "            adjusted_dates.append(new_date.strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "\n",
    "        # Utilisez une boucle pour positionner chaque étiquette\n",
    "    #    for i, label in enumerate(adjusted_dates):\n",
    "     #       ax.text(ticks[i],\n",
    "      #              len(reordered_labels),\n",
    "       #             label,\n",
    "        #            rotation=90,\n",
    "         #           ha='left',\n",
    "          #          va='top',\n",
    "           #         fontname='Liberation Mono')\n",
    "\n",
    "        for i, label in enumerate(adjusted_dates):\n",
    "            # Créez un objet texte (mais ne l'affichez pas encore)\n",
    "            text = ax.text(ticks[i],\n",
    "                        len(reordered_labels),\n",
    "                        label,\n",
    "                        rotation=90,\n",
    "                        ha='left',\n",
    "                        va='top',\n",
    "                        fontname='Liberation Mono')\n",
    "\n",
    "            # Récupérez la largeur du texte en pixels\n",
    "            text_width_px = text.get_window_extent(renderer=fig.canvas.get_renderer()).width\n",
    "\n",
    "            # Convertir la largeur du texte en pixels en coordonnées de données\n",
    "            dx, dy = ax.transData.inverted().transform((text_width_px, 0)) - ax.transData.inverted().transform((0, 0))\n",
    "\n",
    "            # Vérifie si le tick + largeur du texte dépasse les limites de l'axe X\n",
    "            if ticks[i] + (dx + (dx*0.1)) <= ax.get_xlim()[1]:\n",
    "                # Si c'est le cas, affichez le texte\n",
    "                text.set_visible(True)\n",
    "            else:\n",
    "                # Sinon, n'affichez pas le texte\n",
    "                text.set_visible(False)\n",
    "\n",
    "\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "        plt.tight_layout(pad=1.0, h_pad=1.0, w_pad=1.0, rect=[0, 0, 1, 1])\n",
    "\n",
    "        plt.savefig(f\"{results_path}{base_name}_{num_topic}t_{normalize_by_date}dn_{relative_normalizaton}rn_{sigma}s_heatmap.png\",\n",
    "                    dpi=300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llUfNH2qzYDU"
   },
   "outputs": [],
   "source": [
    "def load_documents(name, is_europresse, minimum_caracters_nb_by_document, pbar, lock):\n",
    "  documents = []\n",
    "  all_soups = []\n",
    "  columns_dict = {}\n",
    "\n",
    "  if is_europresse:\n",
    "    document_europresse = ''\n",
    "\n",
    "    with open(name, 'r', encoding='utf-8', errors='xmlcharrefreplace') as file:\n",
    "      for line in file:\n",
    "        document_europresse += line\n",
    "\n",
    "    document_europresse = html.unescape(document_europresse)\n",
    "\n",
    "    document_europresse = document_europresse.replace('</article> <article>', '</article><article>')\n",
    "\n",
    "    documents_europresse = document_europresse.split('</article><article>')\n",
    "\n",
    "    nb_not_occur = 0\n",
    "    for d in documents_europresse:\n",
    "      soup = BeautifulSoup(d, features=\"html.parser\")\n",
    "\n",
    "      with lock:\n",
    "        pbar.update(1)\n",
    "\n",
    "      for p in soup.find_all('p'):\n",
    "        p_text = p.get_text()\n",
    "        if (\"Lire aussi\" in p_text and (\"http\" in p_text or \"https\" in p_text) and len(p_text) <= 1000):\n",
    "          p.decompose()\n",
    "\n",
    "      if len(soup('div', {'class': 'docOcurrContainer'})) > 0:\n",
    "        for p in soup.find_all('p'):\n",
    "          # Trouver le prochain caractère alphabétique après le paragraphe qui n'est pas à l'intérieur d'une balise\n",
    "          next_char_match = re.search(r'(?<=' + re.escape(p.text) + r')\\s*(?:<[^>]*>)*\\s*([a-zA-Z])', str(soup))\n",
    "\n",
    "          # Si le paragraphe ne se termine pas par un point et que le prochain caractère alphabétique est en majuscule\n",
    "          if not p.text.endswith('.') and next_char_match and next_char_match.group(1).isupper():\n",
    "            p.string = p.text + '. '\n",
    "\n",
    "        soup = BeautifulSoup(str(soup), features='html.parser')\n",
    "\n",
    "        candidate_text = soup('div', {'class': 'docOcurrContainer'})[0].get_text()\n",
    "        if len(candidate_text) >= minimum_caracters_nb_by_document and len(candidate_text) < maximum_caracters_nb_by_document:\n",
    "          candidate_text = remove_urls_hashtags_emojis_mentions_emails(candidate_text)\n",
    "          candidate_text = transform_text(candidate_text)\n",
    "          documents.append(candidate_text)\n",
    "          all_soups.append(soup)\n",
    "      else:\n",
    "        nb_not_occur += 1\n",
    "  else:\n",
    "    def detect_delimiter(filename):\n",
    "        with open(filename, 'r', newline='', encoding='utf-8') as file:\n",
    "            sample = file.readline()\n",
    "            sniffer = csv.Sniffer()\n",
    "            return sniffer.sniff(sample).delimiter\n",
    "\n",
    "    delimiter = detect_delimiter(name)\n",
    "\n",
    "    df = pd.read_csv(name, delimiter=delimiter, quotechar='\"')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns={'post created date': 'date'})\n",
    "    df.fillna('', inplace=True)\n",
    "\n",
    "    if 'text' in df:\n",
    "        df = df.loc[(df['text'].str.len() >= minimum_caracters_nb_by_document) & (df['text'].str.len() <= maximum_caracters_nb_by_document)]\n",
    "        documents = df['text'].tolist()\n",
    "    else:\n",
    "        if 'description' in df:\n",
    "            df = df.loc[(df['description'].str.len() >= minimum_caracters_nb_by_document) & (df['description'].str.len() <= maximum_caracters_nb_by_document)]\n",
    "            documents = df['description'].tolist()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(documents):\n",
    "      documents[i] = remove_urls_hashtags_emojis_mentions_emails(documents[i])\n",
    "      documents[i] = transform_text(documents[i])\n",
    "      i += 1\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column != 'text' and column != 'description':\n",
    "            columns_dict[column] = df[column].tolist()\n",
    "\n",
    "  return documents, all_soups, columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp4_SeqfzYDU"
   },
   "outputs": [],
   "source": [
    "def parallel_function(fichier_html, is_europresse, pbar, lock):\n",
    "    return (load_documents(folder_path + 'DATA/' + fichier_html, is_europresse,\n",
    "                           minimum_caracters_nb_by_document, pbar, lock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu6LSdBazYDU"
   },
   "outputs": [],
   "source": [
    "def meta_load_documents():\n",
    "    global documents\n",
    "    global all_soups\n",
    "    global columns_dicts\n",
    "    global is_europresse\n",
    "\n",
    "    # List files that have the base name in them\n",
    "    # Useful for fragmented corpuses\n",
    "    if is_europresse:\n",
    "        fichiers_html = lister_html_insensible(f\"{folder_path}DATA/\")\n",
    "    else:\n",
    "        fichiers_html = [f for f in os.listdir(f\"{folder_path}DATA/\") if f.lower().endswith('.csv') and os.path.isfile(os.path.join(f\"{folder_path}DATA/\", f)) and base_name in f]\n",
    "\n",
    "    # tqdm global for all documents.\n",
    "    pbar = tqdm(bar_format=\"{l_bar}{bar:10}{r_bar}\", position=0, maxinterval=2000, desc='DOCUMENTS PROCESSED')\n",
    "    lock = threading.Lock()  # A lock to prevent conflicts when updating tqdm\n",
    "\n",
    "    all_columns_dicts = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(parallel_function, f, is_europresse, pbar, lock) for f in fichiers_html]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            d, s, cd = future.result()\n",
    "            documents.extend(d)\n",
    "            all_soups.extend(s)\n",
    "            all_columns_dicts.append(cd)\n",
    "\n",
    "    for dico in all_columns_dicts:\n",
    "        for cle, valeur in dico.items():\n",
    "            if cle not in columns_dicts:\n",
    "                columns_dicts[cle] = []\n",
    "            columns_dicts[cle].extend(valeur)\n",
    "\n",
    "   # pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6WxTZ_WzYDU"
   },
   "outputs": [],
   "source": [
    "def update_candidates_for_unigram(kind, unigrams):\n",
    "    global all_tab_pos\n",
    "\n",
    "    if kind == 'np':\n",
    "        all_tab_pos_for_np = copy.deepcopy(all_tab_pos)\n",
    "        i = 0\n",
    "        while i < len(all_tab_pos_for_np):\n",
    "            j = 0\n",
    "            while j < len(all_tab_pos_for_np[i]):\n",
    "                all_tab_pos_for_np[i][j][0] = unidecode.unidecode(all_tab_pos_for_np[i][j][0])\n",
    "                j += 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        all_tab_pos_for_work = all_tab_pos_for_np\n",
    "    else:\n",
    "        all_tab_pos_for_work = all_tab_pos\n",
    "\n",
    "\n",
    "    all_kind = {}\n",
    "    for atb in all_tab_pos_for_work:\n",
    "        for t in atb:\n",
    "            if t[1] == kind:\n",
    "                if t[0] not in all_kind:\n",
    "                    all_kind[t[0]] = []\n",
    "\n",
    "    modes_of_np = {}\n",
    "    for atb in all_tab_pos_for_work:\n",
    "        for t in atb:\n",
    "            if t[0] in all_kind:\n",
    "                all_kind[t[0]].append(t[1])\n",
    "\n",
    "    for e in all_kind:\n",
    "        counts = Counter(all_kind[e])\n",
    "        mode_string, _ = counts.most_common(1)[0]\n",
    "        if mode_string == kind:\n",
    "            unigrams[e] = 1\n",
    "\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8I0UxIrzYDU"
   },
   "outputs": [],
   "source": [
    "def remove_duplicates():\n",
    "    global is_europresse\n",
    "\n",
    "    n_processes = os.cpu_count()\n",
    "\n",
    "    # Prétraitement : Extraire les 100 premiers tokens pour chaque document\n",
    "    # Étape préalable : Diviser chaque document en mots\n",
    "    split_docs = [doc.split() for doc in documents]\n",
    "\n",
    "    # Étape principale : Créer une liste de tuples\n",
    "    tokenized_docs = [(i, doc[:100]) for i, doc in enumerate(split_docs)]\n",
    "\n",
    "    flattened = [word for sublist in split_docs for word in sublist]\n",
    "\n",
    "    # Générer toutes les combinaisons sans répétition\n",
    "    pairs = list(itertools.combinations(tokenized_docs, 2))\n",
    "\n",
    "    # Divisez les paires pour chaque processus\n",
    "    n = len(pairs) // n_processes\n",
    "    sub_pairs = [pairs[i:i+n] for i in range(0, len(pairs), n)]\n",
    "\n",
    "    # Utilisation d'un Manager pour partager la liste des indices entre les processus\n",
    "    with Manager() as manager:\n",
    "        shared_indices = manager.list()\n",
    "\n",
    "        with Pool(n_processes) as pool:\n",
    "            pool.map(check_doublons, [(sp, shared_indices) for sp in sub_pairs])\n",
    "\n",
    "        # Convertissez en set pour supprimer les doublons, puis triez\n",
    "        indices_to_remove = sorted(set(shared_indices), reverse=True)\n",
    "\n",
    "        # Supprimez les doublons en fonction des indices recueillis\n",
    "        for index in indices_to_remove:\n",
    "            del documents[index]\n",
    "\n",
    "            if is_europresse:\n",
    "                del all_soups[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHPgf-mtzYDU"
   },
   "outputs": [],
   "source": [
    "def pre_trim_this_ngram(\n",
    "    wtaan,\n",
    "    cmdaan):\n",
    "\n",
    "  i = 0\n",
    "  while (i < len(cmdaan)\n",
    "        and (\n",
    "          #cmdaan[i] == 'det' or\n",
    "         #    cmdaan[i] == 'other' or\n",
    "         #    cmdaan[i] == 'prep' or\n",
    "          #   cmdaan[i] == 'pron' or\n",
    "             cmdaan[i] == 'sconj' or\n",
    "             cmdaan[i] == 'poncts' or\n",
    "             cmdaan[i] == 'coo' or\n",
    "             ((i+1) < len(cmdaan) and cmdaan[i] == 'adj' and cmdaan[i+1] != 'nc' and cmdaan[i+1] != 'np'))):\n",
    "    i += 1\n",
    "\n",
    "\n",
    "  if i < len(cmdaan):\n",
    "    if i != 0:\n",
    "      j = i\n",
    "      new_aan = []\n",
    "      new_cmdaan = []\n",
    "      while j < len(cmdaan):\n",
    "        new_aan = new_aan + [wtaan[j]]\n",
    "        new_cmdaan = new_cmdaan + [cmdaan[j]]\n",
    "        j += 1\n",
    "\n",
    "      return new_aan, new_cmdaan\n",
    "    else:\n",
    "      return wtaan, cmdaan\n",
    "  else:\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swPTvrPozYDU"
   },
   "outputs": [],
   "source": [
    "def post_trim_this_ngram(\n",
    "    wtaan,\n",
    "    cmdaan):\n",
    "\n",
    "  i = len(cmdaan) - 1\n",
    "  while (i >= 0\n",
    "        and (cmdaan[i] == 'aux'\n",
    "              or cmdaan[i] == 'pron'\n",
    "              or cmdaan[i] == 'coo'\n",
    "             # or cmdaan[i] == 'poncts'\n",
    "              or cmdaan[i] == 'adv'\n",
    "              or cmdaan[i] == 'sconj'\n",
    "              or cmdaan[i] == 'other'\n",
    "              or cmdaan[i] == 'det'\n",
    "              or cmdaan[i] == 'prep'\n",
    "              or ((i-1) >= 0 and cmdaan[i-1] == 'coo' and cmdaan[i] == 'adj')\n",
    "              or ((i-1) >= 0 and cmdaan[i-1] == 'prep' and cmdaan[i] == 'adj')\n",
    "              or ((i-1) >= 0 and cmdaan[i-1] == 'det' and cmdaan[i] == 'adj')\n",
    "              or ((i-1) >= 0 and (cmdaan[i-1] == 'det' or cmdaan[i-1] == 'prep') and cmdaan[i] == 'num'))):\n",
    "    i -= 1\n",
    "\n",
    "  if i > 0:\n",
    "    if i != (len(cmdaan) - 1):\n",
    "      j = 0\n",
    "      new_aan = []\n",
    "      new_cmdaan = []\n",
    "      while j <= i:\n",
    "        new_aan = new_aan + [wtaan[j]]\n",
    "        new_cmdaan = new_cmdaan + [cmdaan[j]]\n",
    "        j += 1\n",
    "\n",
    "      return new_aan, new_cmdaan\n",
    "    else:\n",
    "      return wtaan, cmdaan\n",
    "  else:\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uf1932-_QahN"
   },
   "outputs": [],
   "source": [
    "def pourcentage_mots_en_commun_petit(texte1, texte2):\n",
    "    # Convertir chaque texte en un ensemble de mots\n",
    "    mots_texte1 = set(texte1.split())\n",
    "    mots_texte2 = set(texte2.split())\n",
    "\n",
    "    # Identifier le texte le plus court et le texte le plus long\n",
    "    if len(mots_texte1) < len(mots_texte2):\n",
    "        mots_texte_court = mots_texte1\n",
    "        mots_texte_long = mots_texte2\n",
    "    else:\n",
    "        mots_texte_court = mots_texte2\n",
    "        mots_texte_long = mots_texte1\n",
    "\n",
    "    # Trouver l'intersection des mots entre les deux textes\n",
    "    mots_communs = mots_texte_court.intersection(mots_texte_long)\n",
    "\n",
    "    # Calculer le pourcentage des mots du texte le plus court qui sont présents dans le texte le plus long\n",
    "    pourcentage = (len(mots_communs) / len(mots_texte_court)) * 100\n",
    "\n",
    "    return pourcentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOI-2xmJ6sEf"
   },
   "outputs": [],
   "source": [
    "def pourcentage_mots_en_commun(texte1, texte2):\n",
    "    # Convertir chaque texte en un ensemble de mots\n",
    "    mots_texte1 = set(texte1.split())\n",
    "    mots_texte2 = set(texte2.split())\n",
    "\n",
    "    # Trouver l'intersection des mots entre les deux textes\n",
    "    mots_communs = mots_texte1.intersection(mots_texte2)\n",
    "\n",
    "    # Calculer le pourcentage de mots en commun\n",
    "    total_mots = len(mots_texte1) + len(mots_texte2)\n",
    "    pourcentage = (2 * len(mots_communs) / total_mots) * 100\n",
    "\n",
    "    return pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_joMAvHzYDU"
   },
   "outputs": [],
   "source": [
    "def write_topics_sentences():\n",
    "    for num_topic in relevant_i_by_topics:\n",
    "        tab_words_nmf = []\n",
    "        for _, topic in enumerate(all_nmf_H[num_topic]):\n",
    "            subtab_words_nmf = {}\n",
    "            for i in range(len(topic)):\n",
    "                subtab_words_nmf[tfidf_feature_names[i]] = topic[i]\n",
    "\n",
    "            tab_words_nmf.append(subtab_words_nmf)\n",
    "\n",
    "        topic_dicts = []\n",
    "\n",
    "        i = 0\n",
    "        for tab in tab_words_nmf:\n",
    "            topic_dict = {}\n",
    "            j = 0\n",
    "            for ngram in all_sentences_array[num_topic][i]:\n",
    "                tokens = ngram.split()\n",
    "                total_score = sum(tab.get(token, 0) for token in set(tokens))\n",
    "                topic_dict[all_sentences_array_original[num_topic][i][j]] = total_score\n",
    "\n",
    "                j += 1\n",
    "\n",
    "            topic_dicts.append(topic_dict)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        top_ngrams_per_topic = []\n",
    "\n",
    "        for _, topic_dict in enumerate(topic_dicts):\n",
    "\n",
    "            # 1. Grouper les clés par valeur\n",
    "            value_to_keys = {}\n",
    "            for key, value in topic_dict.items():\n",
    "                if value not in value_to_keys:\n",
    "                    value_to_keys[value] = []\n",
    "                value_to_keys[value].append(key)\n",
    "\n",
    "            # 2. Identifier les clés à supprimer dans chaque groupe\n",
    "            keys_to_remove = set()\n",
    "            for value, keys in value_to_keys.items():\n",
    "                if len(keys) > 1:\n",
    "                    # Trier les clés par longueur\n",
    "                    sorted_keys = sorted(keys, key=lambda x: len(x.split()), reverse=True)\n",
    "                    # Ajouter toutes les clés sauf la plus longue à la liste des clés à supprimer\n",
    "                    keys_to_remove.update(sorted_keys[1:])\n",
    "\n",
    "            # 3. Supprimer les clés identifiées\n",
    "            for key in keys_to_remove:\n",
    "                topic_dict.pop(key, None)\n",
    "\n",
    "            # Le reste du code original\n",
    "            sorted_ngrams = sorted(topic_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            top_ngrams = sorted_ngrams[:1000]\n",
    "\n",
    "            top_ngrams_per_topic.append(top_ngrams)\n",
    "\n",
    "\n",
    "        final_top_ngrams_per_topic = []\n",
    "\n",
    "        for top_ngrams in top_ngrams_per_topic:\n",
    "            final_top_ngrams = []\n",
    "\n",
    "            for ngram, score in top_ngrams:\n",
    "                if len(final_top_ngrams) < 10:\n",
    "                    if all(ngram not in other_ngram for other_ngram, _ in top_ngrams if ngram != other_ngram):\n",
    "                        already_in_approx = False\n",
    "                        for final_ngram in final_top_ngrams:\n",
    "                            if pourcentage_mots_en_commun(final_ngram[0], ngram) >= 70 and pourcentage_mots_en_commun_petit(final_ngram[0], ngram) >= 70:\n",
    "                                already_in_approx = True\n",
    "\n",
    "                        if not already_in_approx:\n",
    "                            good_very_candidate = re.sub(r\"(['’`]) \", r\"\\1\", ngram).replace(' ,', ',').replace(' .', '.')\n",
    "                            count_apos = good_very_candidate.count('\"')\n",
    "\n",
    "                            if count_apos == 1:\n",
    "                                good_very_candidate = good_very_candidate.replace('\"', '', 1)\n",
    "                                good_very_candidate = re.sub(r'\\s+', ' ', good_very_candidate)\n",
    "\n",
    "                            final_top_ngrams.append((good_very_candidate, score))\n",
    "\n",
    "            final_top_ngrams_per_topic.append(final_top_ngrams)\n",
    "\n",
    "        write_sentences_results(topic_model_sentences_output_name + '_' + str(num_topic) + '.csv',\n",
    "                                final_top_ngrams_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipay3YovzYDU"
   },
   "outputs": [],
   "source": [
    "def write_topics_unigrams():\n",
    "    global tf_idf_feature_names\n",
    "    global all_nmf_H\n",
    "    global relevant_i_by_topics\n",
    "\n",
    "    for num_topic in relevant_i_by_topics:\n",
    "        write_unigrams_results(unigrams_nb_by_topic,\n",
    "                        tfidf_feature_names,\n",
    "                        topic_model_unigrams_output_name + '_' + str(num_topic) + '.csv',\n",
    "                        all_nmf_H[num_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GR_CIVCQahN"
   },
   "outputs": [],
   "source": [
    "def sentences_extraction():\n",
    "    global all_sentences_array\n",
    "    global all_sentences_array_original\n",
    "    global sentences_norms\n",
    "    global sentences_lemmas\n",
    "\n",
    "    for num_topic in tqdm(relevant_i_by_topics, desc=\"TOPICS CONFIGURATIONS PROCESSED\", bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        all_sentences_array[num_topic] = {}\n",
    "        all_sentences_array_original[num_topic] = {}\n",
    "\n",
    "        for topic in relevant_i_by_topics[num_topic]:\n",
    "            all_sentences_array[num_topic][topic] = []\n",
    "            all_sentences_array_original[num_topic][topic] = []\n",
    "\n",
    "            i = 0\n",
    "            while i < len(sentences_lemmas):\n",
    "                if i in relevant_i_by_topics[num_topic][topic]:\n",
    "                    all_sentences_array[num_topic][topic].extend(sentences_lemmas[i])\n",
    "                    all_sentences_array_original[num_topic][topic].extend(sentences_norms[i])\n",
    "\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZUKeMCIzYDV"
   },
   "outputs": [],
   "source": [
    "def write_documents_infos():\n",
    "    global every_topics_and_scores_by_document\n",
    "    global is_europresse\n",
    "    global columns_dicts\n",
    "    global all_tab_pos\n",
    "\n",
    "    for num_topic in tqdm(relevant_i_by_topics, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        rows = []\n",
    "        rows_lemmas = []\n",
    "\n",
    "        if is_europresse:\n",
    "            title_row = 'title;authors;raw_authors;nb_characters;journal;date;'\n",
    "            for i in range(num_topic):\n",
    "                title_row += 'topic_' + str(i) + ';' + 'score_' + str(i) + ';'\n",
    "\n",
    "            title_row = title_row[:-1]\n",
    "            rows.append(title_row)\n",
    "\n",
    "            title_row_lemmas = 'lemma;postag_lemma;count_lemma;' + title_row\n",
    "            rows_lemmas.append(title_row_lemmas)\n",
    "        else:\n",
    "            title_row = ''\n",
    "            for key_title in columns_dicts:\n",
    "                title_row += key_title + ';'\n",
    "\n",
    "            title_row += 'nb_characters;'\n",
    "\n",
    "            for i in range(num_topic):\n",
    "                title_row += 'topic_' + str(i) + ';' + 'score_' + str(i) + ';'\n",
    "\n",
    "            title_row = title_row[:-1]\n",
    "            rows.append(title_row)\n",
    "\n",
    "            title_row_lemmas = 'lemma;postag_lemma;count_lemma;' + title_row\n",
    "            rows_lemmas.append(title_row_lemmas)\n",
    "\n",
    "        i = 0\n",
    "        if is_europresse:\n",
    "            while i < len(all_soups):\n",
    "                if i < len(every_topics_and_scores_by_document[num_topic]):\n",
    "                    rows.append(write_info_europresse(every_topics_and_scores_by_document[num_topic][i],\n",
    "                                                    all_soups[i],\n",
    "                                                    documents_lemmatized[i]))\n",
    "\n",
    "                    lemmas_dict = {}\n",
    "                    for unigramme, postag in all_tab_pos[i]:\n",
    "                        if postag == 'np' or postag == 'nc' or postag == 'v':\n",
    "                            unigramme = unigramme.replace('l&apos;', '')\n",
    "                            unigramme = unigramme.replace(';', '')\n",
    "\n",
    "                            if (unigramme, postag) in lemmas_dict:\n",
    "                                lemmas_dict[(unigramme, postag)] += 1\n",
    "                            else:\n",
    "                                lemmas_dict[(unigramme, postag)] = 1\n",
    "\n",
    "                    rows_lemmas.extend(write_info_europresse_lemmas(lemmas_dict,\n",
    "                                                                    every_topics_and_scores_by_document[num_topic][i],\n",
    "                                                                    all_soups[i],\n",
    "                                                                    documents_lemmatized[i]))\n",
    "                i += 1\n",
    "\n",
    "        else:\n",
    "            while i < len(columns_dicts['date']):\n",
    "                rows.append(write_info_another(every_topics_and_scores_by_document[num_topic][i],\n",
    "                                                columns_dicts,\n",
    "                                                i,\n",
    "                                                documents_lemmatized[i]))\n",
    "\n",
    "                lemmas_dict = {}\n",
    "                for unigramme, postag in all_tab_pos[i]:\n",
    "                    if postag == 'np' or postag == 'nc' or postag == 'v':\n",
    "                        unigramme = unigramme.replace('l&apos;', '')\n",
    "                        unigramme = unigramme.replace(';', '')\n",
    "\n",
    "                        if (unigramme, postag) in lemmas_dict:\n",
    "                            lemmas_dict[(unigramme, postag)] += 1\n",
    "                        else:\n",
    "                            lemmas_dict[(unigramme, postag)] = 1\n",
    "\n",
    "                rows_lemmas.extend(write_info_another_lemmas(lemmas_dict,\n",
    "                                                                every_topics_and_scores_by_document[num_topic][i],\n",
    "                                                                columns_dicts,\n",
    "                                                                i,\n",
    "                                                                documents_lemmatized[i]))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        with open(documents_info_name + '_' + str(num_topic) + '.csv', \"w\", encoding='utf-8') as file_object:\n",
    "            for row in rows:\n",
    "                file_object.write(row.replace('\\n', '') + '\\n')\n",
    "\n",
    "        with open(documents_lemmas_info_name + '_' + str(num_topic) + '.csv', \"w\", encoding='utf-8') as file_object:\n",
    "            for row in rows_lemmas:\n",
    "                file_object.write(row.replace('\\n', '') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGhJOx8TDbt0"
   },
   "outputs": [],
   "source": [
    "def write_all_pickles():\n",
    "    with open(f\"{results_path}documents_lemmatized.pkl\", 'wb') as f:\n",
    "        pickle.dump(documents_lemmatized, f)\n",
    "\n",
    "    with open(f\"{results_path}all_tab_pos.pkl\", 'wb') as f:\n",
    "        pickle.dump(all_tab_pos, f)\n",
    "\n",
    "    with open(f\"{results_path}sentences_norms.pkl\", 'wb') as f:\n",
    "        pickle.dump(sentences_norms, f)\n",
    "\n",
    "    with open(f\"{results_path}sentences_lemmas.pkl\", 'wb') as f:\n",
    "        pickle.dump(sentences_lemmas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBhpZCDiDef3"
   },
   "outputs": [],
   "source": [
    "def read_all_pickles():\n",
    "    with open(f\"{results_path}documents_lemmatized.pkl\", 'rb') as f:\n",
    "        documents_lemmatized = pickle.load(f)\n",
    "\n",
    "    with open(f\"{results_path}all_tab_pos.pkl\", 'rb') as f:\n",
    "        all_tab_pos = pickle.load(f)\n",
    "\n",
    "    with open(f\"{results_path}sentences_norms.pkl\", 'rb') as f:\n",
    "        sentences_norms = pickle.load(f)\n",
    "\n",
    "    with open(f\"{results_path}sentences_lemmas.pkl\", 'rb') as f:\n",
    "        sentences_lemmas = pickle.load(f)\n",
    "\n",
    "    return documents_lemmatized, all_tab_pos, sentences_norms, sentences_lemmas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
