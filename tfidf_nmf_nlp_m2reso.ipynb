{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLABAWSWT_kg"
   },
   "source": [
    "# **INITIALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r ./requirements/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1996,
     "status": "ok",
     "timestamp": 1703862134300,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Wqt0GBn7Vpj0",
    "outputId": "67ee81f7-5e2c-4e0d-dc4f-6b884d883634"
   },
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "\n",
    "# à modifier selon utilisation du serveur de google colab (is_local=False)\n",
    "# ou de l'ordinateur sur lequel vous travaillez (is_local=True)\n",
    "is_local = True\n",
    "\n",
    "# pour le moment, seuls le français, l'anglais, l'espagnol, l'allemand,\n",
    "# le catalan, le chinois, le danois, le japonais, le slovaque et l'ukrainien sont gérés.\n",
    "# fr, en, es, de, ca, zh, da, ja, sl, uk\n",
    "language = 'fr'\n",
    "\n",
    "if not is_local:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    folder_path = '/content/drive/MyDrive/COLAB_NLP/'\n",
    "    chdir(folder_path)\n",
    "    %pip install -r ./requirements/requirements.txt\n",
    "else:\n",
    "    folder_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU found\")\n",
    "    spacy.require_gpu()\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1703871256612,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "CfgTHruBWGx0"
   },
   "outputs": [],
   "source": [
    "file_to_run = f\"{folder_path}all_main_functions.ipynb\"\n",
    "%run \"$file_to_run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 34300,
     "status": "ok",
     "timestamp": 1703862168912,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "9VzkN1jJWVvF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go seaborn\n",
      "go gensim\n",
      "go cleantext\n",
      "go adjustText\n",
      "go umap\n",
      "go nltk\n",
      "go sklearn\n",
      "go scipy\n",
      "go matplotlib\n",
      "go spacy\n",
      "go spacy-transformers\n",
      "go pyate\n",
      "go multiprocess\n",
      "go bs4\n",
      "go unidecode\n",
      "go statsmodels\n",
      "Downloading language fr\n",
      "Language fr downloaded\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "\n",
    "import os\n",
    "from urllib import request\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "download_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1703868186358,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "lNnyYagJiERt"
   },
   "outputs": [],
   "source": [
    "imports_file_path = f\"{folder_path}imports.ipynb\"\n",
    "%run \"$imports_file_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703862169687,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "WNcRMegWIasf"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.family'] = 'Liberation Mono'\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "matplotlib.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPy-S4HFDH1P"
   },
   "source": [
    "# **PARAMETERS DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1703868201823,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "bgooqvcWUFJj"
   },
   "outputs": [],
   "source": [
    "# base_name correspond au nom du corpus. c'est la variable qu'il faut modifier de sorte\n",
    "# à caractériser précisément chaque corpus, avec les bonnes pratiques qui conviennent\n",
    "# (par exemple : \"ukraine_russie__presse_francilienne__fev2022_feb2023\").\n",
    "# le contenu de la variable base_name doit être retrouvé à l'intérieur des noms de fichiers,\n",
    "# à l'intérieur du dossier \"DATA\", sur lesquels l'analyse sera produite.\n",
    "# l'algorithme gère les corpus divisés en plusieurs fichiers : il suffit que\n",
    "# le contenu de la variable base_name soit présent dans tous les fichiers concernés par l'analyse.\n",
    "# par exemple, un fichier \"ukraine_russie__presse_francilienne__fev2022_mars2024__feb2022_sep2022.HTML\"\n",
    "# et un fichier \"ukraine_russie__presse_francilienne__fev2022_feb2023__oct2022_feb2023.HTML\"\n",
    "base_name = 'transcripts'\n",
    "\n",
    "results_path = folder_path + \"RESULTS_\" + base_name + \"/\"\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "name_document = f'{base_name}.HTML'\n",
    "raw_documents_output_name = results_path + f'{base_name}_raw_documents.txt'\n",
    "documents_info_name = results_path + f'{base_name}_info'\n",
    "documents_lemmas_info_name = results_path + f'{base_name}_lemmas_info'\n",
    "lemmatized_documents_output_name = results_path + f'{base_name}_lemmatized_documents.txt'\n",
    "\n",
    "topic_model_unigrams_output_name = results_path + f'{base_name}_unigrams_topic_model'\n",
    "topic_model_sentences_output_name = results_path + f'{base_name}_sentences_topic_model'\n",
    "topic_model_terms_output_name = results_path + f'{base_name}_terms_topic_model'\n",
    "topic_model_entities_output_name = results_path + f'{base_name}_entities_topic_model'\n",
    "\n",
    "# ce sont des valeurs qui ne servent que pour les corpus issus d'europresse.\n",
    "# les articles de moins de 500 caractères et de plus de 100000 caractères sont\n",
    "# considérés comme suspicieux, et supprimés.\n",
    "# important à modifier. selon le type de corpus, ces valeurs devraient considérablement varier.\n",
    "# avec des corpus issus d'europresse, l'algorithme supprimerait du corpus tous les articles\n",
    "# avec moins de minimum_caracters_nb_by_document caractères et plus de maximum_caracters_nb_by_document caractères.\n",
    "# il y a à cela deux raisons. d'une part, de la suspicion : qu'est-ce qu'un article avec moins, par exemple, de 500\n",
    "# caractères, et avec plus de 100000 caractères ? ce n'est en tout cas clairement pas standard.\n",
    "# d'autre part, il faut du thème. avec moins de 100 ou 200 caractères, globalement, c'est-à-dire avec moins de vingt mots environ,\n",
    "# il est difficile de qualifier un document avec son vocabulaire seul (c'est le même principe que\n",
    "# lorsqu'on disqualifie un chi2 du fait de croisements dont l'effectif est insuffisant).\n",
    "minimum_caracters_nb_by_document = 250\n",
    "maximum_caracters_nb_by_document = 100000\n",
    "\n",
    "# sub_linear_tfidf est un paramètre aux implications importantes.\n",
    "# avant la factorisation de matrices, grâce à laquelle la détermination des topics/thèmes,\n",
    "# l'algorithme attribue à chaque unigramme lemmatisé un score tf-idf\n",
    "# (term frequency - inverse document frequency).\n",
    "# ce score calcule à quel point un unigramme est présent dans le document,\n",
    "# mais rare dans les autres. c'est un score de spécificité : il approxime à quel point\n",
    "# l'unigramme singularise le document dans lequel il se trouve.\n",
    "# sub_linear_tfidf applique une fonction logarithmique à la fréquence à laquelle\n",
    "# l'unigramme a été trouvé dans le document. le score tf-idf met alors en valeur les mots qui sont spécifiques,\n",
    "# principalement parce qu'ils sont rares dans les autres documents.\n",
    "# le score est alors moins sensible à la fréquence à laquelle l'unigramme est retrouvé dans le document.\n",
    "sub_linear_tfidf = False\n",
    "unigrams_nb_by_topic = 100\n",
    "\n",
    "is_europresse = False\n",
    "\n",
    "# la valeur indique si les doublons doivent être ou non conservés (à False, ils sont conservés ;\n",
    "# à True, ils sont retirés). la modalité \"True\" est probablement importante pour les corpus\n",
    "# issus d'Europresse. nous évitons de cette manière les éventuels problèmes de doublonnage\n",
    "# liés à la manière même dont Europresse fonctionne. nous évitons également les problèmes\n",
    "# possibles de doublonnage au cas où nous ayons par inadvertance des fichiers dont les\n",
    "# articles se recoupent en partie. nous retirons finalement tous les articles produits\n",
    "# par copié/collé d'autres articles. l'algorithme résiste en effet aux variations marginales :\n",
    "# les articles qui n'apportent rien de significativement nouveau sont ainsi supprimés du corpus.\n",
    "# il n'est toutefois pas absurde de vouloir les conserver (le paramètre devra alors être laissé à False).\n",
    "# les copié/collé participent en effet du paysage médiatique auquel nous nous intéressons.\n",
    "go_remove_duplicates = False\n",
    "\n",
    "fontsize = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llQhJTcDDNdk"
   },
   "source": [
    "# **DOCUMENTS PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34881,
     "status": "ok",
     "timestamp": 1703868237462,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "1ABNfjfms9Pk",
    "outputId": "85295f74-acbb-46ab-fe35-d43cea93d203"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DOCUMENTS PROCESSED: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "all_soups = []\n",
    "columns_dicts = {}\n",
    "meta_load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1703868237462,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "5qkvlcVzHKhQ",
    "outputId": "130de63c-3ca5-4733-a374-66b5a1d6fee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "# AFTER OUTLIERS REMOVAL\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6267,
     "status": "ok",
     "timestamp": 1703868243718,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Z7xUWGd0erAl"
   },
   "outputs": [],
   "source": [
    "if go_remove_duplicates:\n",
    "    remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1703868243718,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Iz7XOJQ4rj4w",
    "outputId": "6d5a2f96-699e-4c1a-ff13-42c440058f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "# AFTER DUPLICATES REMOVAL\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1703868244013,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "5GRZMa-bb9KY"
   },
   "outputs": [],
   "source": [
    "write_raw_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y__k8itnLupq"
   },
   "source": [
    "#**DOCUMENTS PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2827369,
     "status": "ok",
     "timestamp": 1703865534866,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "gsEc71wOLQxM",
    "outputId": "87a54183-741c-41bb-ad1b-53438688a1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 4.7481889724731445s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [07:45<00:00,  1.06s/it]  \n"
     ]
    }
   ],
   "source": [
    "documents_lemmatized = []\n",
    "all_tab_pos = []\n",
    "sentences_norms = []\n",
    "sentences_lemmas = []\n",
    "treat_documents_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1703865534867,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "roDPps2YKAf5"
   },
   "outputs": [],
   "source": [
    "write_lemmatized_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1703865536958,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "a0qdPfxcDO0c"
   },
   "outputs": [],
   "source": [
    "write_all_pickles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703865536958,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "6WCpHHA6IRSX",
    "outputId": "e25dda48-4d43-40cb-fe2c-94b8d549f140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "print(len(documents_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVVXtWFYASwW"
   },
   "source": [
    "# **SAVE POINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1703868246987,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "UY1eS7PQDSFf"
   },
   "outputs": [],
   "source": [
    "# possibilité de reprendre l'analyse ici si le traitement a déjà opéré.\n",
    "# il faut toutefois d'abord passer tous les blocs de l'initialisation, et\n",
    "# de la définition du paramètre. la préparation et le processing des documents\n",
    "# peuvent alors être biaisés.\n",
    "documents_lemmatized, all_tab_pos, sentences_norms, sentences_lemmas = read_all_pickles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDqV78DdZDKj"
   },
   "source": [
    "#**UNIGRAMS EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 12428,
     "status": "ok",
     "timestamp": 1703868259397,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "jGQ1qgazf0PC"
   },
   "outputs": [],
   "source": [
    "unigrams = {}\n",
    "unigrams_nouns = {}\n",
    "unigrams_common_nouns = {}\n",
    "unigrams_proper_nouns = {}\n",
    "determine_unigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xQNj6K3ZYL2"
   },
   "source": [
    "#**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 3761,
     "status": "ok",
     "timestamp": 1703868263144,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "QZFa9S_AOMZl"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf, tfidf_feature_names, tokenized_documents = go_tfidf_vectorization(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z70OL7oaZg74"
   },
   "source": [
    "#**UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 88363,
     "status": "ok",
     "timestamp": 1703868351491,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "hlBnvQOilw4c"
   },
   "outputs": [],
   "source": [
    "# unigrams_nb préside au nombre d'unigrammes retenus pour l'analyse.\n",
    "# n_neighbors donne une idée de la contrainte \"structurelle\" :\n",
    "# à quel point umap ira chercher beaucoup\n",
    "create_umap_best_ngrams(unigrams_nb=200, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP_k5UM8lyID"
   },
   "source": [
    "#**NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8741,
     "status": "ok",
     "timestamp": 1703868360220,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "QGZjdjA6WMUO",
    "outputId": "b7f54475-bfdc-4ba0-c1ed-1a9bfcc482dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOPICS CONFIGURATIONS PROCESSED: 100%|██████████| 4/4 [00:00<00:00, 23.46it/s]\n"
     ]
    }
   ],
   "source": [
    "all_nmf_H = {}\n",
    "all_nmf_W = {}\n",
    "relevant_i_by_topics = {}\n",
    "every_topics_and_scores_by_document = {}\n",
    "determine_nmf(initial_topic_num=5, terminal_topic_num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97011,
     "status": "ok",
     "timestamp": 1703868457217,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "9dNCsVROpPc8",
    "outputId": "6503803f-7511-418c-e9d7-609ed26d5493"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 52.74it/s]\n"
     ]
    }
   ],
   "source": [
    "write_documents_infos()\n",
    "write_topics_unigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhzO_mUDmG5y"
   },
   "source": [
    "#**SENTENCES EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1703868458501,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "UCPOeKHoJjir",
    "outputId": "a8732c46-3065-47ed-8db8-671a0e719572"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOPICS CONFIGURATIONS PROCESSED: 100%|██████████| 4/4 [00:00<00:00, 667.09it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentences_array = {}\n",
    "all_sentences_array_original = {}\n",
    "sentences_extraction()\n",
    "write_topics_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l96D3sZHpmmu"
   },
   "source": [
    "#**TERMS AND ENTITIES EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dLQIQQeLjXZN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOPICS CONFIGURATIONS PROCESSED: 100%|██████████| 4/4 [00:37<00:00,  9.42s/it]\n"
     ]
    }
   ],
   "source": [
    "terms_and_entities_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYC2GAShpeHU"
   },
   "source": [
    "#**TOPICS DYNAMICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703868549771,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "1caSJnkIgKq8"
   },
   "outputs": [],
   "source": [
    "original_labels = {5:  [\"0\", \"1\", \"2\", \"3\", \"4\"],\n",
    "                   10: [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n",
    "                   15: [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\"],\n",
    "                   20: [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17754,
     "status": "ok",
     "timestamp": 1703871285383,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "RRwDK00oJjds",
    "outputId": "931192a0-b2a4-47c3-9f28-99847c5c7f2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOPICS CONFIGURATIONS PROCESSED:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOPICS CONFIGURATIONS PROCESSED: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# le paramètre sigma détermine le niveau de lissage. la valeur 'auto' essaie de déterminer\n",
    "# le lissage \"optimal\" à partir de la moyenne des écart-types propres à la distribution de\n",
    "# chaque topic. l'automatisation du lissage ne fonctionne toutefois pas toujours, et il est bon\n",
    "# de tâtonner entres différentes valeurs numériques de sigma (si sigma=1, il n'y a pas de lissage).\n",
    "# il n'y a de toute façon pas de lissage \"optimal\" en soi. il y a bien une sorte de lissage intermédiaire\n",
    "# grâce auquel produire immédiatement des effets d'intelligibilité : c'est le niveau de lissage à partir duquel\n",
    "# se distinguent clairement la dynamique des topics (ceux qui sont surtout représentés au début du corpus, et ceux\n",
    "# qui sont surtout représentés à la fin). il faut toutefois faire attention : ce niveau de lissage tend à masquer de fortes\n",
    "# disparités à l'intérieur de la distribution. une fois la distribution lissée, il n'est plus possible de savoir\n",
    "# si une grande zone forte l'est du fait que le topic soit globalement présent tout du long de la période,\n",
    "# ou si elle l'est du fait d'un ou quelques événements extrêmement intenses.\n",
    "# les deux autres paramètres président à la normalisation de la distribution.\n",
    "# normalize_by_date neutralise dans la distribution les effets dû aux différences du nombre d'articles\n",
    "# publiés à une date donnée. le traitement médiatique de la guerre en ukraine est un exemple impeccable :\n",
    "# sans normalize_by_date=True, tous les topics seraient sur-représentés les premiers jours de la guerre.\n",
    "# c'est un simple effet de la masse de publications ces jours-là. normalize_by_date=True montre ainsi une\n",
    "# une distribution indépendante de la masse de la publication (ce qui ne veut pas dire que l'absence\n",
    "# de normalisation par la masse journalière de publication n'est pas digne d'intérêt).\n",
    "# relative_normalizaton détermine si chaque distribution de chaque topic a sa norme propre,\n",
    "# ou si chaque distribution partage avec les autres une norme d'ensemble. tous les topics ne sont en effet pas\n",
    "# représentés de la même manière dans le corpus. certains sont beaucoup plus présents que d'autres.\n",
    "# de ce fait et en l'absence de normalisation \"relative\", les topics les plus représentés et les moins représentés\n",
    "# s'écraseraient mutuellement. dit autrement : si la différence d'intensité entre les topics les plus représentés\n",
    "# et les topics les moins représentés est trop forte, les topics les plus représentés ne seraient plus que de longues bandes\n",
    "# bleues, et les topics les moins représentés, de longues bandes jaunes. les différences internes à ces distributions\n",
    "# seraient écrasées. la normalisation \"relative\" corrige ce problème de visibilité s'il en est : chaque distribution de\n",
    "# chaque topic est ainsi normalisée par sa norme propre de sorte à ce que pour chaque distribution, la valeur maximale\n",
    "# soit toujours 1, et la valeur minimale, 0. s'observe de cette manière la dynamique \"propre\" du topic.\n",
    "# cela ne veut toujours pas dire que l'absence de normalisation \"relative\" est inintéressante : ne pas normaliser de cette façon\n",
    "# aide à ne pas sur-interpréter les distributions normalisées. si la distribution a été normalisée,\n",
    "#ce n'est pas parce qu'un topic semble fort à un moment donné qu'il l'est dans l'absolu.\n",
    "create_chrono_topics(normalize_by_date=False, relative_normalizaton=True, sigma=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hZ8hwDxVzUZ"
   },
   "source": [
    "#**TOPICS RANKING BARPLOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703868587930,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "-eCIhCv4hFJa",
    "outputId": "89c21a91-8388-46cf-fcc9-19e3f9de35f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 7441,
     "status": "ok",
     "timestamp": 1703868595370,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Weh9EXybJjNh"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# plus une barre est bleue, plus l'écart-type de la distribution à laquelle\n",
    "# elle correspond est faible. autrement dit, plus une barre est bleue, plus\n",
    "# le thème est également distribuée entre les documents. et inversement :\n",
    "# plus une barre est rouge, plus sa distribution est asymétrique. le thème est\n",
    "# très présent sur un ou quelques documents, et très peu voire pas du tout sur les autres.\n",
    "create_topic_barplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdCXtMreVeyH"
   },
   "source": [
    "# **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1703868596313,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Z7xH7AQRxu5g"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m distri_topics_by_journal \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_article \u001b[38;5;129;01min\u001b[39;00m every_topics_and_scores_by_document[num_topic]:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Extrait le nom du journal de l'article et le normalise\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     header \u001b[38;5;241m=\u001b[39m \u001b[43mall_soups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_article\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mheader\n\u001b[0;32m     11\u001b[0m     journal_text \u001b[38;5;241m=\u001b[39m extract_information(header, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.rdp__DocPublicationName\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     journal_text \u001b[38;5;241m=\u001b[39m normalize_journal(journal_text)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Structure modifiée pour une meilleure lisibilité et maintenance\n",
    "distri_topics_by_journal_by_num_topic = {}\n",
    "\n",
    "for num_topic in every_topics_and_scores_by_document:\n",
    "    # Initialiser un dictionnaire pour stocker la distribution des sujets par journal pour ce topic\n",
    "    distri_topics_by_journal = {}\n",
    "\n",
    "    for num_article in every_topics_and_scores_by_document[num_topic]:\n",
    "        # Extrait le nom du journal de l'article et le normalise\n",
    "        header = all_soups[num_article].header\n",
    "        journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "        journal_text = normalize_journal(journal_text)\n",
    "\n",
    "        # Itère sur chaque tuple sujet-score dans l'article\n",
    "        for topic_score_tuple in every_topics_and_scores_by_document[num_topic][num_article]:\n",
    "            topic, score = topic_score_tuple  # Décomposer le tuple\n",
    "\n",
    "            # Met à jour le dictionnaire pour ce sujet\n",
    "            if topic not in distri_topics_by_journal:\n",
    "                distri_topics_by_journal[topic] = {}\n",
    "\n",
    "            if journal_text not in distri_topics_by_journal[topic]:\n",
    "                distri_topics_by_journal[topic][journal_text] = []\n",
    "\n",
    "            distri_topics_by_journal[topic][journal_text].append(score)\n",
    "\n",
    "    # Met à jour le dictionnaire principal avec la distribution des sujets pour ce topic\n",
    "    distri_topics_by_journal_by_num_topic[num_topic] = distri_topics_by_journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR9-2G2yVQr-"
   },
   "source": [
    "# **KRUSKAL-WALLIS TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703868596313,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "goHk4fOyynLF"
   },
   "outputs": [],
   "source": [
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703868596313,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "vaJA1E6MAgZu"
   },
   "outputs": [],
   "source": [
    "kruskal_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 268255,
     "status": "ok",
     "timestamp": 1703868864566,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "zr39M37HY4Mo"
   },
   "outputs": [],
   "source": [
    "for num_topic in distri_topics_by_journal_by_num_topic:\n",
    "    for topic in distri_topics_by_journal_by_num_topic[num_topic]:\n",
    "        topic_data = distri_topics_by_journal_by_num_topic[num_topic][topic]\n",
    "\n",
    "        # Collecte des données de score pour chaque journal sous ce sujet et topic\n",
    "        data = []\n",
    "        journals = []  # Pour les étiquettes\n",
    "        for journal, scores in topic_data.items():\n",
    "            data.extend(scores)\n",
    "            journals.extend([journal] * len(scores))\n",
    "\n",
    "        # Création d'un DataFrame pour Seaborn\n",
    "        df = pd.DataFrame({'Journal': journals, 'Score': data})\n",
    "\n",
    "        # Compter les occurrences de chaque journal\n",
    "        journal_counts = df['Journal'].value_counts()\n",
    "\n",
    "        # Filtrer les journaux avec moins de \"threshold\" occurrences\n",
    "        journals_to_keep = journal_counts[journal_counts >= threshold].index\n",
    "        df = df[df['Journal'].isin(journals_to_keep)]\n",
    "\n",
    "        # Vérifie s'il y a au moins deux groupes pour effectuer le test\n",
    "        if len(set(df['Journal'])) < 2:\n",
    "            print(\"Pas assez de groupes pour effectuer le test pour ce sujet et topic\")\n",
    "            continue\n",
    "\n",
    "        # Effectuer le test de Kruskal-Wallis\n",
    "        stat, p = stats.kruskal(*[group['Score'] for name, group in df.groupby('Journal')])\n",
    "\n",
    "        kruskal_results[(num_topic, topic)] = {'statistic': stat, 'p_value': p}\n",
    "\n",
    "        unique_journals = df['Journal'].unique()\n",
    "        max_ylim = [float('inf'), float('-inf')]  # Initialiser avec des valeurs extrêmes\n",
    "\n",
    "        # Première itération pour déterminer la largeur nécessaire et les limites de l'axe des y\n",
    "        for journal in unique_journals:\n",
    "            journal_data = df[df['Journal'] == journal]['Score']\n",
    "            ax_temp = sns.violinplot(y=journal_data) #, ax=axes[i])\n",
    "            ylim = ax_temp.get_ylim()\n",
    "            max_ylim = [min(max_ylim[0], ylim[0]), max(max_ylim[1], ylim[1])]  # Mettre à jour les limites de l'axe des y\n",
    "            plt.close()  # Fermer le plot temporaire\n",
    "\n",
    "        max_ylim[1] = max_ylim[1] - (max_ylim[1] / 5)\n",
    "\n",
    "\n",
    "       # sns.set_theme(context='notebook', style='white')\n",
    "\n",
    "        # Créer la figure et le GridSpec\n",
    "        fig = plt.figure(figsize=(15, 10), constrained_layout=True) #(20*(math.sqrt(len(unique_journals)/3)), 7))\n",
    "\n",
    "        # Ajout du titre\n",
    "        fig.suptitle(f\"diagrammes en violon pour le topic '{original_labels[num_topic][topic]}' \"\n",
    "                    f\"(configuration à {num_topic} topics) \\n statistique de kruskal-wallis={stat:.3f}, \"\n",
    "                    f\"p={p:.3f}\")\n",
    "\n",
    "        gs = GridSpec(1, len(unique_journals))\n",
    "\n",
    "        # Calculer la moyenne des scores pour chaque journal\n",
    "        mean_scores = df.groupby('Journal')['Score'].mean()\n",
    "\n",
    "        # Normaliser les moyennes des scores\n",
    "        normalized_mean_scores = (mean_scores - mean_scores.min()) / (mean_scores.max() - mean_scores.min())\n",
    "\n",
    "        # Créer la palette de couleurs\n",
    "        palette = sns.color_palette(\"flare\", as_cmap=True)\n",
    "\n",
    "        # Appliquer la palette de couleurs aux moyennes des scores normalisées\n",
    "        mean_score_colors = normalized_mean_scores.apply(lambda x: palette(x))\n",
    "\n",
    "        # Tracer les plots avec des largeurs ajustées et des limites de l'axe des y uniformes\n",
    "        for i, journal in enumerate(unique_journals):\n",
    "            ax = fig.add_subplot(gs[i])\n",
    "            journal_data = df[df['Journal'] == journal]['Score']\n",
    "            color = mean_score_colors[journal]\n",
    "            sns.violinplot(y=journal_data,\n",
    "                           ax=ax,\n",
    "                           cut=0,\n",
    "                           color=color,\n",
    "                           linewidth=0,\n",
    "                           width=1.0,\n",
    "                           saturation=1.0,\n",
    "                           gridsize=1000)\n",
    "            ax.set_ylim(max_ylim)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_xlabel('')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('')\n",
    "            else:\n",
    "                ax.set_ylabel('')\n",
    "                ax.set_yticklabels([])\n",
    "\n",
    "            # Désactiver les bordures du plot\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "            # Filtrer pour n'afficher que les ticks positifs sur l'axe des y\n",
    "            yticks = ax.get_yticks()\n",
    "            ax.set_yticks([ytick for ytick in yticks if ytick >= 0])\n",
    "\n",
    "            # Ajouter les noms des journaux à la verticale\n",
    "            ax.annotate(journal, xy=(0, 0), xytext=(0, 0),\n",
    "                        textcoords='offset points', ha='center', va='top', rotation=90)\n",
    "\n",
    "#        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    " #       plt.tight_layout(pad=1.0, h_pad=1.0, w_pad=1.0, rect=[0, 0, 1, 1])\n",
    "\n",
    "        plt.savefig(f\"{results_path}{base_name}_{num_topic}nt_{topic}t_{threshold}thres_journals_violin_plots.png\",\n",
    "                    dpi=300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0)\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1703868864882,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "YvyLSg7KBJV4"
   },
   "outputs": [],
   "source": [
    "# Conversion en DataFrame\n",
    "kruskal_df = pd.DataFrame([(key[0], key[1], val['statistic'], val['p_value']) for key, val in kruskal_results.items()],\n",
    "                          columns=['Num_Topic', 'Topic', 'Statistique', 'P_value'])\n",
    "\n",
    "# Tri et formatage\n",
    "kruskal_df.sort_values(by='Statistique', ascending=False, inplace=True)\n",
    "kruskal_df['Statistique'] = kruskal_df['Statistique'].round(3)\n",
    "kruskal_df['P_value'] = kruskal_df['P_value'].apply(lambda x: f\"{x:.3e}\" if x < 0.001 else f\"{x:.3f}\")\n",
    "\n",
    "# Pivoter le DataFrame pour la heatmap\n",
    "heatmap_data = kruskal_df.pivot(columns=[\"Num_Topic\", \"Topic\", \"Statistique\"])\n",
    "\n",
    "# Création de la heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=False, cmap=\"coolwarm\", linewidths=.5)\n",
    "\n",
    "plt.title(\"heatmap des statistiques de kruskal-wallis\")\n",
    "plt.ylabel(\"configurations en nombres de topics\")\n",
    "plt.xlabel(\"numéros des topics\")\n",
    "\n",
    "plt.savefig(f\"{results_path}{base_name}_{threshold}thres_kruskal_heatmap.png\")\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hES99gFeVTZT"
   },
   "source": [
    "# **CHI2 TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1703868866146,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "srQvHBSlQz4k"
   },
   "outputs": [],
   "source": [
    "chi2_results = []\n",
    "\n",
    "# Traitement pour chaque num_topic\n",
    "for num_topic in every_topics_and_scores_by_document.keys():\n",
    "    data = []\n",
    "\n",
    "    # Parcourir les articles de ce num_topic\n",
    "    for num_article, topic_scores in every_topics_and_scores_by_document[num_topic].items():\n",
    "        # Trouver le topic avec le score le plus élevé pour cet article\n",
    "        highest_score_topic = max(topic_scores, key=lambda item: item[1])[0]\n",
    "\n",
    "        # Extraire le nom du journal\n",
    "        header = all_soups[num_article].header\n",
    "        journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "        journal_text = normalize_journal(journal_text)\n",
    "\n",
    "        # Ajouter au tableau de données\n",
    "        data.append([journal_text, highest_score_topic])\n",
    "\n",
    "    # Créer un DataFrame pour ce num_topic\n",
    "    df = pd.DataFrame(data, columns=[\"Journal\", \"Topic\"])\n",
    "\n",
    "    # Compter les occurrences de chaque journal\n",
    "    journal_counts = df['Journal'].value_counts()\n",
    "\n",
    "    # Filtrer les journaux avec moins de occurrences que le nombre déterminé par le seuil\n",
    "    journals_to_keep = journal_counts[journal_counts >= threshold].index\n",
    "    df = df[df['Journal'].isin(journals_to_keep)]\n",
    "\n",
    "    # Si pas assez de journaux, passer à l'itération suivante\n",
    "    if len(journals_to_keep) < 2:\n",
    "        print(f\"num_topic: {num_topic} - Pas assez de journaux avec au moins 10 occurrences pour effectuer le test\")\n",
    "        continue\n",
    "\n",
    "    # Créer un tableau de contingence pour ce num_topic\n",
    "    contingency_table = pd.crosstab(df['Journal'], df['Topic'])\n",
    "\n",
    "    #print(contingency_table)\n",
    "\n",
    "    # Exécuter le test du chi2 pour ce num_topic\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Ajouter les résultats dans la liste\n",
    "    chi2_results.append((num_topic, chi2, p))\n",
    "\n",
    "# Créer un DataFrame à partir de la liste des résultats\n",
    "results_df = pd.DataFrame(chi2_results, columns=[\"topics_nb\", \"chi2\", \"p_value\"])\n",
    "\n",
    "# Enregistrer le DataFrame dans un fichier CSV\n",
    "results_df.to_csv(f\"{results_path}{base_name}_{threshold}thres_chi2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pyuYNNBVaqW"
   },
   "source": [
    "# **MULTINOMIAL REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1703868866362,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "7z8Mer78mqmc"
   },
   "outputs": [],
   "source": [
    "# Itération sur chaque topic et chaque article\n",
    "for num_topic, articles in every_topics_and_scores_by_document.items():\n",
    "    if num_topic == 5:\n",
    "        data_for_regression = []\n",
    "\n",
    "        for num_article, topics_scores in articles.items():\n",
    "            # Extraction et normalisation du nom du journal\n",
    "            header = all_soups[num_article].header\n",
    "            journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "            journal_text = normalize_journal(journal_text)\n",
    "\n",
    "            # Création de l'entrée pour cet article\n",
    "            article_data = {'journal': journal_text}\n",
    "            article_data.update(topics_scores)\n",
    "            data_for_regression.append(article_data)\n",
    "\n",
    "        # Conversion en DataFrame\n",
    "        df = pd.DataFrame(data_for_regression)\n",
    "\n",
    "        # Gestion des valeurs manquantes\n",
    "        df.fillna(0, inplace=True)  # Remplace les valeurs manquantes par 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703868866363,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "b4FPcEYRonGp"
   },
   "outputs": [],
   "source": [
    "# Vérification de la Multicollinéarité\n",
    "# Calcul du VIF (Facteur d'Inflation de la Variance)\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703868866363,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "tmMox9_MtUTO"
   },
   "outputs": [],
   "source": [
    "multinomial_regression_num_topic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14983,
     "status": "ok",
     "timestamp": 1703868881344,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "d45VcLxBosWx",
    "outputId": "066aa502-8cb4-45a6-919f-cc591133461b"
   },
   "outputs": [],
   "source": [
    "# Itération sur chaque topic et chaque article\n",
    "for num_topic, articles in every_topics_and_scores_by_document.items():\n",
    "    if num_topic == multinomial_regression_num_topic:\n",
    "        data_for_regression = []\n",
    "\n",
    "        for num_article, topics_scores in articles.items():\n",
    "            # Extraction et normalisation du nom du journal\n",
    "            header = all_soups[num_article].header\n",
    "            journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "            journal_text = normalize_journal(journal_text)\n",
    "\n",
    "            # Création de l'entrée pour cet article\n",
    "            article_data = {'journal': journal_text}\n",
    "            article_data.update(topics_scores)\n",
    "            data_for_regression.append(article_data)\n",
    "\n",
    "        # Conversion en DataFrame\n",
    "        df = pd.DataFrame(data_for_regression)\n",
    "\n",
    "        # Gestion des valeurs manquantes\n",
    "        df.fillna(0, inplace=True)  # Remplace les valeurs manquantes par 0\n",
    "\n",
    "        # Calculer la fréquence de chaque journal\n",
    "        frequences = df['journal'].value_counts()\n",
    "\n",
    "        # Identifier les journaux peu représentés\n",
    "        journaux_peu_representes = frequences[frequences < threshold].index\n",
    "\n",
    "        # Supprimer les lignes contenant ces journaux peu représentés\n",
    "        df = df[~df['journal'].isin(journaux_peu_representes)]\n",
    "\n",
    "        # Histogrammes pour les Variables Numériques\n",
    "        df.hist(figsize=(12, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        compte_classes = df['journal'].value_counts()\n",
    "\n",
    "        # Calculer les proportions\n",
    "        proportions_classes = compte_classes / len(df)\n",
    "\n",
    "        # Afficher les compte et proportions\n",
    "        print(\"Compte des classes:\")\n",
    "        print(compte_classes)\n",
    "        print(\"\\nProportions des classes:\")\n",
    "        print(proportions_classes)\n",
    "\n",
    "        # Visualiser la distribution des classes\n",
    "        plt.bar(compte_classes.index, compte_classes.values)\n",
    "        plt.xlabel('Classe')\n",
    "        plt.ylabel('Nombre d’occurrences')\n",
    "        plt.title('Distribution des Classes')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        # Création de variables dummy pour les journaux\n",
    "        journal_dummies = pd.get_dummies(df['journal'])\n",
    "        df = pd.concat([df, journal_dummies], axis=1)\n",
    "\n",
    "        print('REMAINING DOCUMENTS NB', len(df))\n",
    "\n",
    "\n",
    "        # Définition des variables indépendantes et dépendantes\n",
    "        X = df.drop(['journal'] + list(journal_dummies.columns), axis=1)  # Scores des topics\n",
    "\n",
    "        vif_data = calculate_vif(X)\n",
    "        print(vif_data)\n",
    "\n",
    "        y = df[list(journal_dummies.columns)]  # Journaux en tant que variables dummy\n",
    "\n",
    "        # Ajout d'une constante à X pour le terme d'intercept\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Construction et ajustement du modèle de régression multinomiale\n",
    "        model = sm.MNLogit(y, X)\n",
    "        result = model.fit(method='bfgs', maxiter=5000, tol=1e-5)\n",
    "\n",
    "        # Affichage des résultats\n",
    "       # print(result.summary())\n",
    "\n",
    "        # Supposons que 'result' est l'objet contenant les résultats de votre régression logistique multinomiale\n",
    "        summary = result.summary()\n",
    "\n",
    "        # Enregistrer le résumé dans un fichier texte\n",
    "        with open(f\"{results_path}{base_name}_{num_topic}t_regression_full_summary.txt\", 'w') as fh:\n",
    "            fh.write(summary.as_text())\n",
    "\n",
    "        # Enregistrer le tableau de résultats dans un fichier CSV\n",
    "        with open(f\"{results_path}{base_name}_{num_topic}t_{threshold}thres_regression_results.csv\", 'w') as fh:\n",
    "            fh.write(summary.tables[1].as_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p20j1mPIVxx2"
   },
   "source": [
    "# **SENTIMENT ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1703868881344,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "6DKl4kIaQ2uQ"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1703868881344,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "16CW5XuaRKD9"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('transformers')\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3003,
     "status": "ok",
     "timestamp": 1703868884336,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "xxK0yghL-tYr"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment' # Modèle spécifique pour l'analyse de sentiments\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iue9AWZ23-vI"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        # Tokeniser le texte pour compter les tokens\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # Diviser le texte en segments, en respectant la limite de tokens\n",
    "        max_tokens = 400 - tokenizer.num_special_tokens_to_add()  # Compte pour [CLS] et [SEP]\n",
    "        token_chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "        # Convertir les segments de tokens en chaînes de caractères\n",
    "        text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
    "\n",
    "        # Analyser le sentiment de chaque segment avec une barre de progression\n",
    "        results = []\n",
    "        for chunk in text_chunks:\n",
    "            segment_result = sentiment_pipeline(chunk)\n",
    "            results.append(segment_result)\n",
    "\n",
    "        # Combiner les résultats\n",
    "        # La logique de combinaison dépend de vos besoins spécifiques\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sentiment analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Imaginons que 'documents' est votre tableau de textes\n",
    "sentiments = [analyze_sentiment(doc) for doc in tqdm(documents, desc=\"Processing Documents\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149224,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "e-vh-3xWtM1e"
   },
   "outputs": [],
   "source": [
    "def extract_and_convert_date(date_str):\n",
    "    # Utilisation d'une expression régulière pour extraire la date\n",
    "    match = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_str)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d/%m/%Y')\n",
    "    else:\n",
    "        # Gérer les cas où aucune date n'est trouvée\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149224,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "Wm_bnJXGWDyh"
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "if is_europresse:\n",
    "    for soup in all_soups:\n",
    "        header = soup\n",
    "        date_text = extract_information(header, '.DocHeader')\n",
    "        date_text_clean = extract_date_info(date_text)\n",
    "        date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "        date_normalized = extract_date_from_text(date_normalized)\n",
    "        dates.append(date_normalized)\n",
    "else:\n",
    "    from dateutil.parser import parse\n",
    "\n",
    "    def detecter_date(chaine, jour_en_premier=True):\n",
    "        try:\n",
    "            return parse(chaine, dayfirst=jour_en_premier)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def formater_date(date):\n",
    "        return date.strftime('%d/%m/%Y')\n",
    "\n",
    "    def formater_liste_dates(liste_dates, jour_en_premier=True):\n",
    "        return [formater_date(detecter_date(date_str, jour_en_premier)) for date_str in liste_dates if detecter_date(date_str, jour_en_premier)]\n",
    "\n",
    "    dates = formater_liste_dates(columns_dicts['date'])\n",
    "\n",
    "# Filtrer et convertir les dates\n",
    "new_dates = []\n",
    "for date_str in dates:\n",
    "    date = extract_and_convert_date(date_str)\n",
    "    new_dates.append(date)\n",
    "\n",
    "dates = new_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149224,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "EfFx9voV3-tE"
   },
   "outputs": [],
   "source": [
    "# Transformer les sentiments en scores basés sur les étoiles\n",
    "transformed_sentiments = []\n",
    "for doc_sentiments in sentiments:\n",
    "    if doc_sentiments:  # Assurez-vous que la liste n'est pas vide\n",
    "        doc_scores = []\n",
    "        for sentiment_list in doc_sentiments:\n",
    "            sentiment = sentiment_list[0]  # Accéder au premier élément de la liste\n",
    "            label = sentiment['label']\n",
    "            star_rating = int(label.split()[0])  # Extraire le nombre d'étoiles\n",
    "\n",
    "            doc_scores.append(star_rating)\n",
    "\n",
    "        average_score = sum(doc_scores) / len(doc_scores)\n",
    "        transformed_sentiments.append(average_score)\n",
    "    else:\n",
    "        transformed_sentiments.append(None)  # ou une valeur par défaut\n",
    "\n",
    "# Supposons que `dates` est votre liste de dates sous forme de chaînes de caractères\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'date': dates, 'sentiment': transformed_sentiments})\n",
    "\n",
    "# Convertir les chaînes de caractères en type datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Trier le DataFrame par date\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "# Grouper par date et calculer la moyenne des sentiments par jour\n",
    "df_grouped = df.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le_IhhxAYZ39"
   },
   "source": [
    "# **SENTIMENT OVER TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149224,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "TouNMOIX3-q0"
   },
   "outputs": [],
   "source": [
    "# Supposons que 'Valeur' est le nom de la colonne de données que vous voulez lisser\n",
    "y = df_grouped['sentiment'].values\n",
    "x = df_grouped.index.values\n",
    "\n",
    "# Appliquer le filtre de Savitzky-Golay\n",
    "window_size = 41  # La taille de la fenêtre doit être impaire\n",
    "polynomial_order = 3  # L'ordre du polynôme utilisé pour l'ajustement\n",
    "yhat = savgol_filter(y, window_size, polynomial_order)\n",
    "\n",
    "# Créer le graphique\n",
    "#plt.plot(x, y, label='Original')\n",
    "plt.plot(x, yhat, color='blue')\n",
    "plt.title('dynamique des sentiments')\n",
    "\n",
    "plt.xticks(rotation=90)  # Rotation des étiquettes de l'axe x à 90 degrés\n",
    "\n",
    "plt.savefig(f\"{results_path}{base_name}_{num_topic}t_{window_size}ws_{polynomial_order}po_sentiments_over_time.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149224,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "jyEa9GWy3-o7"
   },
   "outputs": [],
   "source": [
    "# Calcul du poids total de chaque topic par jour pour chaque topic_count\n",
    "total_weight_by_topic_count_topic_and_date = {}\n",
    "for topic_count, articles in every_topics_and_scores_by_document.items():\n",
    "    for article_num, topic_scores in articles.items():\n",
    "        article_date = dates[article_num]\n",
    "        for topic_num, topic_weight in topic_scores:\n",
    "            key = (topic_count, topic_num, article_date)\n",
    "            if key not in total_weight_by_topic_count_topic_and_date:\n",
    "                total_weight_by_topic_count_topic_and_date[key] = topic_weight\n",
    "            else:\n",
    "                total_weight_by_topic_count_topic_and_date[key] += topic_weight\n",
    "\n",
    "# Initialisation d'un dictionnaire pour stocker les sentiments normalisés par date, topic_count et topic_num\n",
    "sentiment_by_date_and_topic = {}\n",
    "\n",
    "for topic_count, articles in every_topics_and_scores_by_document.items():\n",
    "    for article_num, topic_scores in articles.items():\n",
    "        article_date = dates[article_num]  # Date de l'article\n",
    "        sentiment_score = transformed_sentiments[article_num]  # Score de sentiment de l'article\n",
    "\n",
    "        for topic_num, topic_weight in topic_scores:\n",
    "            # Ajustement du score de sentiment par le poids du topic dans l'article\n",
    "            adjusted_sentiment_score = sentiment_score * topic_weight\n",
    "\n",
    "            # Clé pour le poids total du topic par jour pour un topic_count donné\n",
    "            weight_key = (topic_count, topic_num, article_date)\n",
    "            if total_weight_by_topic_count_topic_and_date[weight_key] > 0:\n",
    "                normalized_sentiment_score = adjusted_sentiment_score / total_weight_by_topic_count_topic_and_date[weight_key]\n",
    "            else:\n",
    "                normalized_sentiment_score = 0\n",
    "\n",
    "            # Clé unique pour la combinaison topic_count, topic_num et date\n",
    "            combined_key = (topic_count, topic_num, article_date)\n",
    "\n",
    "            if combined_key not in sentiment_by_date_and_topic:\n",
    "                if normalized_sentiment_score < 0:\n",
    "                    sentiment_by_date_and_topic[combined_key] = [-math.sqrt(abs(normalized_sentiment_score))]\n",
    "                else:\n",
    "                    sentiment_by_date_and_topic[combined_key] = [math.sqrt(abs(normalized_sentiment_score))]\n",
    "            else:\n",
    "                if normalized_sentiment_score < 0:\n",
    "                    sentiment_by_date_and_topic[combined_key].append(-math.sqrt(abs(normalized_sentiment_score)))\n",
    "                else:\n",
    "                    sentiment_by_date_and_topic[combined_key].append(math.sqrt(abs(normalized_sentiment_score)))\n",
    "\n",
    "# Calcul de la moyenne des sentiments normalisés pour chaque combinaison de topic_count, topic_num et date\n",
    "for key, normalized_sentiments in sentiment_by_date_and_topic.items():\n",
    "    average_sentiment = sum(normalized_sentiments) # / len(normalized_sentiments)\n",
    "    sentiment_by_date_and_topic[key] = average_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-6opV8RYeoo"
   },
   "source": [
    "# **SENTIMENT OVER TIME BY TOPIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "2DaORFBW31VZ"
   },
   "outputs": [],
   "source": [
    "relative_normalizaton = False\n",
    "sigma = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "O6q9jPaN_wnt"
   },
   "outputs": [],
   "source": [
    "for topic_count, articles in every_topics_and_scores_by_document.items():\n",
    "    # Filtrer les données pour un topic_count spécifique\n",
    "    filtered_data = {(topic_num, date): sentiment for (count, topic_num, date), sentiment\n",
    "                     in sentiment_by_date_and_topic.items() if count == topic_count}\n",
    "    # Création d'un DataFrame pour les données filtrées\n",
    "    df = pd.DataFrame(list(filtered_data.items()), columns=['Topic_Date', 'Sentiment'])\n",
    "    df[['Topic', 'Date']] = pd.DataFrame(df['Topic_Date'].tolist(), index=df.index)\n",
    "    # Pivoter le DataFrame pour la heatmap\n",
    "    #print(df)\n",
    "    df = df.pivot(columns=\"Date\", index=\"Topic\", values=\"Sentiment\")\n",
    "    #print(df)\n",
    "\n",
    "    df.interpolate(method='linear', axis=1, inplace=True)\n",
    "\n",
    "    list_of_series = []\n",
    "\n",
    "    # Application du filtre gaussien\n",
    "    for index, row in df.iterrows():\n",
    "        filtered_values = gaussian_filter(row, sigma=sigma)\n",
    "\n",
    "        s = pd.Series(filtered_values, index=df.columns, name=index)\n",
    "        list_of_series.append(s)\n",
    "\n",
    "    df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "    if relative_normalizaton:\n",
    "        list_of_series = []\n",
    "\n",
    "        for index, row in df_normalized.iterrows():\n",
    "            normalized_values = (row - row.min()) / (row.max() - row.min())\n",
    "            s = pd.Series(normalized_values, index=df_normalized.columns, name=index)\n",
    "            list_of_series.append(s)\n",
    "\n",
    "        df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "    dist_matrix = cosine_distances(df_normalized.values)\n",
    "    condensed_dist_matrix = squareform(dist_matrix)\n",
    "\n",
    "    Z = linkage(condensed_dist_matrix, method='ward', optimal_ordering=True)   # method='ward')\n",
    "    dendro = dendrogram(Z, no_plot=True)\n",
    "    df_normalized = df_normalized.iloc[dendro['leaves']]\n",
    "    reordered_labels = [original_labels[topic_count][i] for i in np.array(df_normalized.index)]\n",
    "\n",
    "    df_normalized.columns = pd.to_datetime(df_normalized.columns).date\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    ax = sns.heatmap(df_normalized, cmap=\"coolwarm\", cbar=False)\n",
    "    ax.set_yticklabels(reordered_labels, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.savefig(f\"{results_path}{base_name}_{topic_count}t_{relative_normalizaton}rn_{sigma}s_sentiments_heatmap.png\",\n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fLM8ogbsJBp"
   },
   "source": [
    "# **STACKED AREA CHARTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "AFV6RGC9xkIm"
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_stacked_area_chart(topics_data, dates):\n",
    "    # Initialiser une liste pour stocker les DataFrames temporaires\n",
    "    temp_dfs = []\n",
    "\n",
    "    # Itérer sur chaque article en vérifiant les limites du tableau dates\n",
    "    for article_num, topics in topics_data.items():\n",
    "        # Vérifier si l'index est dans les limites du tableau dates\n",
    "        if article_num < len(dates):\n",
    "            # Obtenir la date correspondante\n",
    "            date = dates[article_num]\n",
    "\n",
    "            # Créer un DataFrame temporaire pour cet article\n",
    "            temp_df = pd.DataFrame([score for _, score in topics],\n",
    "                                   index=[topic for topic, _ in topics],\n",
    "                                   columns=[date]).T\n",
    "\n",
    "            # Ajouter le DataFrame temporaire à la liste\n",
    "            temp_dfs.append(temp_df)\n",
    "\n",
    "    # Concaténer tous les DataFrames temporaires\n",
    "    df = pd.concat(temp_dfs)\n",
    "\n",
    "    # Regrouper par date et sommer les scores pour chaque topic\n",
    "    df = df.groupby(level=0).sum()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "hlwBJ5Jz7E17"
   },
   "outputs": [],
   "source": [
    "def apply_weighted_smoothing(df, column_name):\n",
    "    n = len(df[column_name])\n",
    "    # Define points where the weight starts to increase/decrease\n",
    "    lower_bound = int(n * 0.3)\n",
    "    upper_bound = int(n * 0.7)\n",
    "\n",
    "    # Initialize weights to 1\n",
    "    weights = np.ones(n)\n",
    "\n",
    "    # Set weights: from 1 to 0 for the first 30%, remain at 0 up to 70%, then from 0 back to 1\n",
    "    weights[:lower_bound] = np.linspace(1, 0, lower_bound)\n",
    "    weights[lower_bound:upper_bound] = 0\n",
    "    weights[upper_bound:] = np.linspace(0, 1, n - upper_bound)\n",
    "\n",
    "    # Calculate the weighted rolling means\n",
    "    smooth_data = pd.Series(df[column_name]).rolling(window=lower_bound, min_periods=1).mean()\n",
    "\n",
    "    # Apply the custom weights to have a progressive smoothing towards the 30% mark from the ends\n",
    "    # The middle values (30% to 70%) are not smoothed at all (weight of 0 on the smoothed values)\n",
    "    smooth_data = smooth_data * weights + df[column_name] * (1 - weights)\n",
    "    return smooth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "0ECm2lzvSaER"
   },
   "outputs": [],
   "source": [
    "deg = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "dQPJN7I9xLbA"
   },
   "outputs": [],
   "source": [
    "for i_test in range(1):\n",
    "    for topic_count in every_topics_and_scores_by_document:\n",
    "        df_topics = prepare_data_for_stacked_area_chart(every_topics_and_scores_by_document[topic_count],\n",
    "                                                        dates)\n",
    "        df_topics.index = pd.to_datetime(df_topics.index)\n",
    "\n",
    "        df_resampled = df_topics.resample('D').asfreq()\n",
    "\n",
    "        # Interpolez les valeurs manquantes linéairement\n",
    "        df_topics = df_resampled.interpolate(method='linear')\n",
    "\n",
    "        # Création d'un nouveau DataFrame pour stocker les courbes ajustées\n",
    "        df_smooth = pd.DataFrame(index=df_topics.index)\n",
    "\n",
    "        x_data = np.linspace(0, 1, len(df_topics))\n",
    "\n",
    "        for column in df_topics.columns:\n",
    "            # Ajuster le polynôme aux données\n",
    "            coeffs = np.polyfit(x_data, df_topics[column], deg)\n",
    "\n",
    "            # Évaluer le polynôme ajusté\n",
    "            df_smooth[column] = np.polyval(coeffs, x_data)\n",
    "\n",
    "        dist_matrix = cosine_distances(df_smooth.transpose().values)\n",
    "        cosine_dist_matrix = squareform(dist_matrix)\n",
    "\n",
    "        # Effectuer le clustering hiérarchique à l'aide de la matrice de distances\n",
    "        Z = linkage(cosine_dist_matrix, method='complete', optimal_ordering=True)\n",
    "\n",
    "        # Obtenir l'ordre des feuilles de l'arbre de clustering\n",
    "        leaf_order = leaves_list(Z)\n",
    "\n",
    "        # Convertir leaf_order en noms de colonnes\n",
    "        column_names_ordered = df_smooth.columns[leaf_order]\n",
    "\n",
    "        # Réorganiser df_smooth en utilisant les noms des colonnes\n",
    "        df_smooth = df_smooth[column_names_ordered]\n",
    "\n",
    "        nb_try = 0\n",
    "        while (df_smooth < 0).any().any() and nb_try < 500:\n",
    "            nb_try += 1\n",
    "\n",
    "            df_smooth = df_smooth.clip(lower=0)\n",
    "\n",
    "            # Remplacer les valeurs NaN par 0 avant la normalisation si nécessaire\n",
    "            df_smooth.fillna(0, inplace=True)\n",
    "\n",
    "            if i_test == 0:\n",
    "                # Calculer la somme de chaque ligne\n",
    "                row_sums = df_smooth.sum(axis=1)\n",
    "\n",
    "                # Éviter la division par zéro en remplaçant les sommes nulles par 1\n",
    "                # Cela évite de diviser par zéro si une ligne entière est composée de zéros\n",
    "                row_sums[row_sums == 0] = 1\n",
    "\n",
    "                df_smooth = df_smooth.div(row_sums,\n",
    "                                        axis=0)\n",
    "\n",
    "            df_smooth2 = pd.DataFrame(index=df_topics.index)\n",
    "\n",
    "            x_data = np.linspace(0,\n",
    "                                1,\n",
    "                                len(df_topics))\n",
    "\n",
    "            for column in df_smooth.columns:\n",
    "                # Ajuster le polynôme aux données\n",
    "                coeffs = np.polyfit(x_data,\n",
    "                                    df_smooth[column],\n",
    "                                    deg)\n",
    "\n",
    "                # Évaluer le polynôme ajusté\n",
    "                df_smooth2[column] = np.polyval(coeffs,\n",
    "                                                x_data)\n",
    "\n",
    "            df_smooth = df_smooth2\n",
    "\n",
    "\n",
    "        #df_smooth = df_smooth.apply(lambda col: apply_weighted_smoothing(df_smooth, col.name))\n",
    "\n",
    "\n",
    "        # Tracer le graphique en aires empilées avec les données lissées\n",
    "        fig, ax = plt.subplots(figsize=(20, 12))\n",
    "        palette = sns.color_palette(\"pastel\")\n",
    "        stacks = ax.stackplot(df_topics.index,\n",
    "                            df_smooth.T,\n",
    "                            edgecolor='white',\n",
    "                            linewidth=0.5,\n",
    "                            colors=palette)\n",
    "\n",
    "        colors = [stack.get_facecolor()[0] for stack in stacks]\n",
    "\n",
    "        # Ajouter les noms des topics sur les aires\n",
    "        cumulative_heights = np.cumsum(df_smooth.values,\n",
    "                                    axis=1)\n",
    "        x_label = df_topics.index[0]  # Position x pour les labels\n",
    "\n",
    "        x_label_num = mdates.date2num(df_topics.index[0])\n",
    "\n",
    "        original_labels_reordered = [original_labels[topic_count][name]\n",
    "                                     for name in column_names_ordered]\n",
    "\n",
    "        all_y = []\n",
    "        all_topics = []\n",
    "        for i, topic in enumerate(original_labels_reordered):\n",
    "            all_topics.append(topic)\n",
    "            all_y.append((cumulative_heights[0, i] +\n",
    "                          cumulative_heights[0, i - 1]) / 2\n",
    "                         if i > 0 else cumulative_heights[0, i] / 2)\n",
    "\n",
    "        everything_fine = False\n",
    "        while not everything_fine:\n",
    "            everything_fine = True\n",
    "\n",
    "            i = 1\n",
    "            while i < len(all_y):\n",
    "                correction = False\n",
    "                while (all_y[i] - all_y[i - 1]) < 0.022:\n",
    "                    all_y[i - 1] = all_y[i - 1] - 0.00001\n",
    "                    all_y[i] = all_y[i] + 0.00001\n",
    "                    correction = True\n",
    "\n",
    "                if correction:\n",
    "                    everything_fine = False\n",
    "\n",
    "                i += 1\n",
    "\n",
    "\n",
    "        offset_x = int(len(dates) / 10)\n",
    "        offset_x = 0\n",
    "        i = 0\n",
    "        while i < len(all_y):\n",
    "            ax.text(x_label_num - offset_x,\n",
    "                    all_y[i],\n",
    "                    all_topics[i],\n",
    "                    ha='right',\n",
    "                    va='center',\n",
    "                    color='black',\n",
    "                    fontname='Liberation Mono',\n",
    "                    bbox=dict(facecolor=colors[i], pad=0.0, edgecolor='none', alpha=0.35))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Calcul de l'intervalle dynamique pour les dates\n",
    "        interval = calculate_date_interval(df_topics,\n",
    "                                        figsize=(20, 12))\n",
    "\n",
    "        # Configuration de l'axe des abscisses avec l'intervalle dynamique\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=interval))\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ax.set_xlim(df_topics.index.min(), df_topics.index.max())\n",
    "\n",
    "        ticks = ax.get_xticks()\n",
    "        labels = [date.strftime('%d-%m-%Y') for date in mdates.num2date(ticks)]\n",
    "\n",
    "        ax.set_xticks([])  # Supprimer les marqueurs de l'axe des y\n",
    "        ax.set_yticks([])  # Supprimer les marqueurs de l'axe des y\n",
    "        ax.set_xlabel('')  # Supprimer le titre de l'axe des x\n",
    "        ax.set_ylabel('')  # Supprimer le titre de l'axe des y\n",
    "        ax.set_title('')  # Supprimer le titre du graphique\n",
    "\n",
    "        cumulative_max = df_smooth.sum(axis=1).max()\n",
    "\n",
    "        # Utilisez une boucle pour positionner chaque étiquette\n",
    "        for i, label in enumerate(labels):\n",
    "            ax.text(ticks[i],\n",
    "                    0,\n",
    "               #     -0.013*cumulative_max,\n",
    "                    label,\n",
    "                    rotation=90,\n",
    "                    ha='left',\n",
    "                    va='top',\n",
    "                    fontname='Liberation Mono')\n",
    "\n",
    "        plt.ylim(top=cumulative_max)\n",
    "\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "        plt.tight_layout(pad=1.0, h_pad=1.0, w_pad=1.0, rect=[0, 0, 1, 1])\n",
    "\n",
    "        if i_test == 0:\n",
    "            plt.savefig(f\"{results_path}{base_name}_{topic_count}t_\" \\\n",
    "                        f\"{deg}d_100_stacked_area_chart.png\",\n",
    "                        dpi=300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0)\n",
    "        else:\n",
    "            plt.savefig(f\"{results_path}{base_name}_{topic_count}t_\" \\\n",
    "                        f\"{deg}d_stacked_area_chart.png\",\n",
    "                        dpi=300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0)\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u2ibzPQVQD0"
   },
   "source": [
    "# **ENTROPY DYNAMICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "GDjz6n8YxkBn"
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(words):\n",
    "    word_counts = Counter(words)\n",
    "    total_words = sum(word_counts.values())\n",
    "    entropy = -sum((count/total_words) * math.log2(count/total_words) for count in word_counts.values())\n",
    "    return entropy\n",
    "\n",
    "for topic_count in every_topics_and_scores_by_document:\n",
    "    # Charger les données\n",
    "    data = pd.read_csv(f\"{results_path}{base_name}_lemmas_info_{topic_count}.csv\", sep=';')\n",
    "\n",
    "    # Convertir les dates en objets de date avec le format jour/mois/année\n",
    "    data['date'] = pd.to_datetime(data['date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Convertir les dates en périodes temporelles hebdomadaires\n",
    "    data['week'] = data['date'].dt.to_period('W')\n",
    "\n",
    "    # Calculer l'entropie pour chaque semaine\n",
    "    entropies = data.groupby('week')['lemma'].apply(calculate_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "y29-WMj0dt50"
   },
   "outputs": [],
   "source": [
    "if pd.NaT in entropies.index:\n",
    "    entropies = entropies.drop(pd.NaT)\n",
    "\n",
    "# Vérifier si l'index est déjà un timestamp\n",
    "if entropies.index.dtype != 'datetime64[ns]':\n",
    "    entropies.index = entropies.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "fLMotQPugeJ5"
   },
   "outputs": [],
   "source": [
    "window_size = 2  # La taille de la fenêtre peut être ajustée selon vos besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "2LByEGDzogXQ"
   },
   "outputs": [],
   "source": [
    "rolling_entropies = entropies.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "\n",
    "# Plotting the data using seaborn\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.lineplot(data=rolling_entropies)\n",
    "\n",
    "# Personnalisation\n",
    "plt.title(f\"entropie hebdomadaire des mots - lissage avec moyenne mobile sur une fenêtre de {window_size}\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"entropie moyenne\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.savefig(f\"{results_path}{base_name}_lemmas_entropy_week_based_{window_size}ws.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QTDTgkFatMT"
   },
   "source": [
    "# **TOPICS CLUSTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13132,
     "status": "ok",
     "timestamp": 1703872231737,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "wvSX5vulxj_r"
   },
   "outputs": [],
   "source": [
    "for topic_count in every_topics_and_scores_by_document:\n",
    "    similarity_matrix = cosine_similarity(all_nmf_H[topic_count])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(similarity_matrix)\n",
    "\n",
    "    # Calcul de la distance euclidienne par rapport au centre [0, 0]\n",
    "    distances = np.sqrt(np.sum(pca_result ** 2, axis=1))\n",
    "\n",
    "    # Créer une colormap pour les couleurs en fonction de la distance\n",
    "    colormap = plt.cm.Blues\n",
    "\n",
    "    # Normaliser les distances pour les utiliser comme valeurs de couleur\n",
    "    norm = plt.Normalize(vmin=distances.min(), vmax=distances.max())\n",
    "    colors = colormap(norm(distances))\n",
    "\n",
    "    # Récupérer les scores de résumé de la variance (explained variance ratio)\n",
    "    explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Visualisation avec des couleurs en fonction de la distance\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=colors)\n",
    "\n",
    "    labels = [f'{i}' for i in range(len(pca_result))]\n",
    "\n",
    "    texts = [plt.text(topic[0], topic[1], original_labels[topic_count][int(label)]) for topic, label in zip(pca_result, labels)]\n",
    "\n",
    "    adjust_text(texts, force_points=0.2, force_text=0.2, expand_text=(1.2, 1.2))\n",
    "\n",
    "    # Afficher les scores de résumé de la variance sur les axes\n",
    "    plt.title(f\"ACP des distances cosine inter-topics\\nbasées sur les vecteurs d'unigrammes\")\n",
    "    plt.xlabel(f'facteur 1 - variance expliquée={explained_var_ratio[0]*100:.2f}%')\n",
    "    plt.ylabel(f'facteur 2 - variance expliquée={explained_var_ratio[1]*100:.2f}%')\n",
    "\n",
    "    # Supprimer la partie haute et droite du cadre\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{results_path}{base_name}_{topic_count}nc_inter_topics_cosine_pca.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "seJzAcBihNls"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1703869149225,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "5_HR7bj5kbvp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
